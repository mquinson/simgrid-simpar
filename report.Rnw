\documentclass{article}

\begin{document}
\SweaveOpts{concordance=TRUE}

\section{Problem Analysis}
\subsection{Optimal Amdhal's law threshold}
We want to find the maximum speedup achieved with our current parallel model. For that, a test a benchmark test is run to get the timings of a typical sequential and parallel executions. After that, and using the Amdahl's law, we can retrieve the real speedup achieved with our system.
But first we want to prove that our benchmarks are not intrusive, that is, our measures do no really affect the overall performance of the system. For that, the experiments are run with and without benchmarking.
Then, taking a typical Chord simulation, a comparison of both versions (benchmarked one and original) is made to find if there is a significative breach in the timings.
The percentage difference equation is used to represent the how big is the breach between the two experiments:
\[
\frac{|benchmark-raw|}{(benchmark+raw)/2}
\]
Being \emph{raw} the time of a typical sequential simulation and \emph{benchmark} the time of a simulation with benchmarks enabled.

%<<BenchmarkNotIntrusive,echo = FALSE,fig = TRUE>>=
%library('ggplot2')
%library('gridExtra')
%library('reshape')
%tim = read.table("./optimizations_experiments/timings/timings_d321137.dat")
%tim_amdahl = read.table("./optimizations_experiments/timings/timings_Amdahl_d321137.dat")
%tim = as.data.frame.matrix(tim)
%tim_amdahl = as.data.frame.matrix(tim_amdahl)
%data <- data.frame(nodes =  tim[1:4,1], constant=tim[1:4,2],constant_bench=tim_amdahl[1:4,2])
%df <- melt(data ,  id = 'nodes', variable_name = 'versions')
%g <- ggplot(df, aes(x=nodes,y=value, group=versions, colour=versions)) + geom_line() + scale_fill_hue()
%plot(g)
%makeFootnote("Timings of a tipical run vs. a run with benchmarks activated", color = "black")
%@

<<PercentageDifference,echo = FALSE,fig = TRUE>>=
library('ggplot2')
library('gridExtra')
library('reshape')
# write a simple function to add footnote
makeFootnote <- function(footnoteText =
                         format(Sys.time(), "%d %b %Y"),
                         size = 0.8, color = grey(.5))
{
  require(grid)
  pushViewport(viewport())
  grid.text(label = footnoteText ,
    x = unit(0.5,"npc"),
    y = unit(0.5, "mm"),
    just =c("centre", "bottom"),
    check.overlap = 1,
    gp = gpar(cex = size, col = color))
 popViewport()
}

orig_data = read.table("./optimizations_experiments/timings/total_times_noamdahl.log")
opt_data = read.table("./optimizations_experiments/timings/total_sum_times_amdahl.log")
orig_data = as.data.frame.matrix(orig_data)
opt_data = as.data.frame.matrix(opt_data)
data <- data.frame(nodes =  orig_data[1:4,1], 
                   t2nobench = orig_data[1:4,9],
                   t8nobench = orig_data[1:4,11],
                   t2bench = opt_data[1:4,9],
                   t8bench = opt_data[1:4,11])
data[, "diff2"] <- abs(data$t2nobench - data$t2bench)
data[, "diff8"] <- abs(data$t8nobench - data$t8bench)
data[, "sum2"] <- data$t2nobench + data$t2bench
data[, "sum8"] <- data$t8nobench + data$t8bench
data[, "avg2"] <- data$sum2 / 2
data[, "avg8"] <- data$sum8 / 2
data[, "pdiff2"] <- data$diff2 / data$avg2
data[, "pdiff8"] <- data$diff8 / data$avg8
data[, "diff2"] <- NULL
data[, "diff8"] <- NULL
data[, "sum2"] <- NULL
data[, "sum8"] <- NULL
data[, "avg2"] <- NULL
data[, "avg8"] <- NULL
data[, "t2nobench"] <- NULL
data[, "t8nobench"] <- NULL
data[, "t2bench"] <- NULL
data[, "t8bench"] <- NULL
df <- melt(data ,  id = 'nodes', variable_name = 'difference')
g <- ggplot(df, aes(x=nodes,y=value, group=difference, colour=difference)) + geom_line() + scale_fill_hue() + ylim(0,0.3)
plot(g)
makeFootnote("Figure 1: Percentage difference of time between benchmarked and original version", color = "black")
@
As it can be seen in the Figure 1, the maximum difference in the execution time of both versions is 15\%, which is not big enough to say that the benchmark is intrusive\\
TODO: This is just an asumption. I think I need a stronger statistical fact here to prove what I want. Also, is the percentage diff equation the best option?

As a conclusion, the benchmarking is not refally intrusive for the computation, since it does not affect the parallel nor sequential executions of the simulation in a significant way.

To calculate the speedup obtained with our parallel model, the experiment was ran with 1000, 3000, 5000 and 10000 nodes. But as it can be seen in the Figure, the real speedup is achieved with bigger sizes.\\
TODO: is practically impossible to run simulation with 100k nodes, thows segfault. Without that, there's no way to show that there is speedup.

<<AmdahlSpeedup,echo = FALSE,fig = TRUE>>=
library('ggplot2')
library('gridExtra')
library('reshape')
# write a simple function to add footnote
makeFootnote <- function(footnoteText =
                         format(Sys.time(), "%d %b %Y"),
                         size = 0.8, color = grey(.5))
{
  require(grid)
  pushViewport(viewport())
  grid.text(label = footnoteText ,
    x = unit(0.5,"npc"),
    y = unit(0.5, "mm"),
    just =c("centre", "bottom"),
    check.overlap = 1,
    gp = gpar(cex = size, col = color))
 popViewport()
}

orig_data = read.table("./optimizations_experiments/timings/total_times_noamdahl.log")
opt_data = read.table("./optimizations_experiments/timings/total_sum_times_amdahl.log")
orig_data = as.data.frame.matrix(orig_data)
opt_data = as.data.frame.matrix(opt_data)
data <- data.frame(nodes =  orig_data[1:4,1], seq = orig_data[1:4,8], sumamd = opt_data[1:4,11])
data[, "speedup"] <- data[, "seq"] / data[, "sumamd"]
df <- melt(data ,  id = 'nodes', variable_name = 'speedup')
g <- ggplot(data, aes(x=nodes,y=speedup, colour=speedup)) + geom_line() + scale_colour_continuous(guide=FALSE)
#labs(title = 'Chord simulation, Precise Model', sub = 'Original version vs. Adaptative algorithm', ylab = 'time')
plot(g)
makeFootnote("Figure 2: Real speedup achieved using parallel mode in Chord simulation.", color = "black")
@


\subsection{Parallelizable portions of the problem}
(Probably a comment of scheduling rounds and parallel/sequential execution.)
This experiment will be based on a typical Chord simulation, and the data wanted are the following: ID of each Scheduling Round, time taken by each Scheduling Round and number of process executed in each scheduling round.

What we want to prove is that the limit on the speed up reached is due to the fact that we are very closer to the limit of parallelizable portions of the system. As it can be seen on the first graph, the amount of processes computed by each scheduling round is only one most of the times, so the parallel execution is not possible in that instances. The remaining processes are executed in parallel due to the parallel execution threshold already setted up in SimGrid (which can be modified).

<<ShedRoundDistribution,echo = FALSE,fig = TRUE>>=
library('ggplot2')
library('gridExtra')
library('reshape')
makeFootnote <- function(footnoteText =
                         format(Sys.time(), "%d %b %Y"),
                         size = 0.8, color = grey(.5))
{
  require(grid)
  pushViewport(viewport())
  grid.text(label = footnoteText ,
    x = unit(0.5,"npc"),
    y = unit(0.5, "mm"),
    just =c("centre", "bottom"),
    check.overlap = 1,
    gp = gpar(cex = size, col = color))
 popViewport()
}
sr_data = read.table("./optimizations_experiments/sr_counts/sr_total_1000.log")
sr_data = as.data.frame.matrix(sr_data)
ggplot(data=sr_data, geom="histogram", aes(x=V3)) + xlim(0,13) + geom_histogram(aes(y=..count../sum(..count..))) + xlab("") + ylab("")

makeFootnote("Amount of scheduling rounds computing.", color = "black")
@


<<SchedRoundTimes,echo = FALSE,fig = TRUE>>=
library('ggplot2')
library('gridExtra')
makeFootnote <- function(footnoteText =
                         format(Sys.time(), "%d %b %Y"),
                         size = 0.8, color = grey(.5))
{
  require(grid)
  pushViewport(viewport())
  grid.text(label = footnoteText ,
    x = unit(0.5,"npc"),
    y = unit(0.5, "mm"),
    just =c("centre", "bottom"),
    check.overlap = 1,
    gp = gpar(cex = size, col = color))
 popViewport()
}
temp = list.files(path='./optimizations_experiments/sr_counts/', pattern="*precise.log", full.names = TRUE)
flist <- lapply(temp, read.table)
sr_data <-rbindlist(flist)
sr_data[, "V1"] <- NULL
sr_data = as.data.frame.matrix(sr_data)

#for the mean
library('plyr')
df <- ddply(sr_data, .(V3), summarize, mean_value = mean(V2))
g <-ggplot(data=df, geom="histogram", aes(x=V3, y=mean_value)) + xlab("") + ylab("") + geom_line()
plot(g)
makeFootnote("Mean of times depending on the amount of processes of each scheduling round.", color = "black")
@

\subsection{}

\section{Optimal threshold for parallel execution}
\subsection{Adaptative algorithm to calculate threshold}
But finding an optimal threshold is not always the best option: some simulations can take more time in each process and other less time. If a simulation has very efficient processes, or processes that dont work too much, then the threshold could be inefficient,
Thats why an algorithm for a dynamic threshold calculation is proposed.
TODO: explanation of the heuristic... is the amount of time taken by each scheduling round, and calculate on the fly a dynamic threshold to fit better the simulation. Pseudocode maybe

\subsection{Models used in the chord simulation}
\begin{itemize}
\item Workstation model: Default vm workstation model (as it appears on ./chord --help)
\item Network Model: LV08 (or Constant)
\item Cpu Model: Cas01
\end{itemize}
<<AdaptativeAlgorithmPlot,echo = FALSE,fig = TRUE>>=
library('ggplot2')
library('gridExtra')
library('reshape')
# write a simple function to add footnote
makeFootnote <- function(footnoteText =
                         format(Sys.time(), "%d %b %Y"),
                         size = 0.8, color = grey(.5))
{
  require(grid)
  pushViewport(viewport())
  grid.text(label = footnoteText ,
    x = unit(0.5,"npc"),
    y = unit(0.5, "mm"),
    just =c("centre", "bottom"),
    check.overlap = 1,
    gp = gpar(cex = size, col = color))
 popViewport()
}

orig_data = read.table("./optimizations_experiments/dynamic_threshold/optimization3.dat")
opt_data = read.table("./optimizations_experiments/dynamic_threshold/optimization3_part2.dat")
orig_data = as.data.frame.matrix(orig_data)
opt_data = as.data.frame.matrix(opt_data)
#data <- data.frame(nodes =  orig_data[1:4,1], thr4const=orig_data[1:4,2], thr8const=orig_data[1:4,3], thr16const=orig_data[1:4,4], thr4prec=orig_data[1:4,5],thr8prec=orig_data[1:4,6],thr16prec=orig_data[1:4,7],optthr4const=opt_data[1:4,2], optthr8const=opt_data[1:4,3], optthr16const=opt_data[1:4,4], optthr4prec=opt_data[1:4,5], optthr8prec=opt_data[1:4,6],optthr16prec=opt_data[1:4,7])
data <- data.frame(nodes =  orig_data[1:4,1], thr4prec=orig_data[1:4,5],thr8prec=orig_data[1:4,6],thr16prec=orig_data[1:4,7],optthr4prec=opt_data[1:4,5], optthr8prec=opt_data[1:4,6],optthr16prec=opt_data[1:4,7])
df <- melt(data ,  id = 'nodes', variable_name = 'versions')
g <- ggplot(df, aes(x=nodes,y=value, group=versions, colour=versions)) + geom_line() + scale_fill_hue()
#labs(title = 'Chord simulation, Precise Model', sub = 'Original version vs. Adaptative algorithm', ylab = 'time')
plot(g)
makeFootnote("Chord simulation, Precise Model. Original version vs. Adaptative algorithm", color = "black")
@

%\subsection{Finding performance problems origins}

%<<FaultyCommit,echo = FALSE,fig = TRUE>>=
%library('ggplot2')
%library('gridExtra')
%library('reshape')
%# write a simple function to add footnote
%makeFootnote <- function(footnoteText =
%                         format(Sys.time(), "%d %b %Y"),
%                         size = 0.8, color = grey(.5))
%{
%  require(grid)
%  pushViewport(viewport())
% grid.text(label = footnoteText ,
%   x = unit(0.5,"npc"),
%    y = unit(0.5, "mm"),
%   just =c("centre", "bottom"),
%    check.overlap = 1,
%    gp = gpar(cex = size, col = color))
% popViewport()
%}

%pinpoint = read.table("./optimizations_experiments/pinpoint/pinpoint.dat")
%pinpoint = as.data.frame.matrix(pinpoint)
%#data <- data.frame(nodes =  orig_data[1:4,1], thr4const=orig_data[1:4,2], thr8const=orig_data[1:4,3], thr16const=orig_data[1:4,4], thr4prec=orig_data[1:4,5],thr8prec=orig_data[1:4,6],thr16prec=orig_data[1:4,7],optthr4const=opt_data[1:4,2], optthr8const=opt_data[1:4,3], optthr16const=opt_data[1:4,4], optthr4prec=opt_data[1:4,5], optthr8prec=opt_data[1:4,6],optthr16prec=opt_data[1:4,7])
%data <- data.frame(hashs =  pinpoint[1:15,1], thr4_prec=pinpoint[1:15,4], thr8_prec=pinpoint[1:15,5])
%df <- melt(data ,  id = 'hashs', variable_name = 'versions')
%df <- transform(df, hashs = factor(hashs, levels = c("34de819","71c9241","f95108e","8ec4cdb","34e46b4","49288d9","a711dd8","ccfa6d0","58edfe4","048b8c7","b9734b1","edd6d5b","525d972","eb547ff","ce1289d")))
%g <- ggplot(df, aes(x=hashs,y=value, group=versions, colour=versions)) + geom_line() + scale_fill_hue() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
%#labs(title = 'Chord simulation, Precise Model', sub = 'Original version vs. Adaptative algorithm', ylab = 'time')
%plot(g)
%#makeFootnote("Finding faulty commit. Chord Simulation", color = "black")
%@

\section{Optimizations}
\subsection{}
\subsection{Parmap between N cores}
 \#proc. (1-5 --> sequential; 6-20 --> 4busy waiters; 20-inf --> 16 busy waiter).
\subsection{Busy Waiters}
 \#proc. (1-5 --> sequential; 6-20 --> 4busy waiters; 20-inf --> 16 busy waiter).
 \subsection{Performance Regression Testing}
\end{document}
