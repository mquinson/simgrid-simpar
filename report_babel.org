#+TITLE: Internship Report
#+AUTHOR: Ezequiel Torti Lopez
#+OPTIONS: num:nil ^:nil f:nil
#+LATEX_HEADER: \documentclass{article}
#+LATEX_HEADER: \usepackage{amscd}
#+LATEX_HEADER: \usepackage{wrapfig}
#+STARTUP: hideblocks
#+PROPERTY: session *R* 

#+begin_LaTeX
  \hypersetup{
    linkcolor=blue,
    pdfborder={0 0 0 0}
  }
  \lstset{basicstyle=\ttfamily\bfseries\small}
#+end_LaTeX

#+begin_src R  :exports none
library('ggplot2')
library('gridExtra')
library('reshape')
library('plyr')
library('data.table')
#+end_src

#+LaTeX: \begin{document}


#+LaTeX: \section{Motivation and Problem Statement}

Simulation is the third pillar of science, allowing to study complicated
phenomenons through complex models. When the size or complexity of the studied
models becomes too large, it is classical to leverage more resources through
Parallel Discrete-Event Simulation (PDES).  

Still, the parallel simulation of very fine grained applications deployed on
large-scale distributed systems (LSDS) remains challenging. As a matter of fact,
most simulators of Peer-to-Peer systems are sequential, despite the vast
literature on PDES over the last three decades.

dPeerSim is one of the very few existing PDES for P2P systems, but it presents
deceiving performance: it can achieve a decent speedup when increasing the
amount of logical processes (LP): from 4h with 2 LPs down to 1h with 16 LPs.
But it remains vastly inefficient when compared to sequential version of
PeerSim, that performs the same experiment in 50 seconds only. This calls for a
new parallel schema specifically tailored to this category of Discrete Event
Simulators.

Discrete Event Simulation of Distributed Applications classically alternates
between simulation phases where the models compute the next event date, and
phases where the application workload is executed.  We proposed
in~\cite{previous} to not split the simulation model across several computing
nodes, but instead to keep the model sequential and execute the application
workload in parallel when possible. We hypothesized that this would help
reducing the synchronization costs. We evaluate our contribution with very fine
grained workloads such as P2P protocols. These workloads are the most difficult
to execute efficiently in parallel because execution times are very short,
making it very difficult to amortize the synchronization times.

We implemented this parallel schema within the SimGrid framework, and showed
that the extra complexity does not endangers the performance since the
sequential version of SimGrid still outperforms several competing solutions when
our addition are present but disabled at run time.

To the best of our knowledge, it is the first time that a parallel simulation of
P2P system proves to be faster that the best known sequential execution. Yet,
the parallel simulation only outperforms sequential one when the amount of
processes becomes large enough. This is because of the pigonhole principle: when
the amount of processes increases, the average amount of processes that are
ready to run at each simulated timestamp (and can thus run in parallel)
increases. When simulating the Chord protocol, it takes 500,000 processes or
more to amortizing the synchronization costs, while the classical studies of the
literature usually involve less processes.

The current work aims at further improving the performance of our PDES, using
several P2P protocols as a workload. We investigate the possible inefficiency
and propose generic solutions that could be included in other similar simulators
of large-scale distributed systems, be them P2P simulators of cloud, HPC or
sensornets ones.

This paper is organized as follows: Section~\ref{sec:context} recaps the SimGrid
architecture and quickly presents the parallel execution schema detailed
in~\cite{previous}. Section~\ref{sec:parallel} explores several trade-offs for
the efficiency of the parallel sections. Section~\ref{sec:problem} analysis the
theoretical performance bound, and discusses the previous work at the light of
the Amhdal law.  Section~\ref{sec:adaptative} proposes an algorithm to
automatically tune the level of parallelism that is adapted to the simulated
application. Section~\ref{sec:cc} concludes this paper and discusses some future
work.


#+LaTeX: \section{Context}\label{sec:context}

In the previous work ~\cite{previous} we proposed to parallelize the execution
of the user code while keeping the simulation engine sequential.
This is enabled by applying classical concepts of OS design to this new context:
every interaction between the user processes (from now on, user processes and
processes mean the same thing) and the simulated environment passes
through a specific layer that act as an OS kernel.

A novel way to virtualize user processes (\emph{raw contexts}) was
crafted to improve efficiency and avoid unnecesary system calls, 
but other ways to do this can be found for the sake of portability, such as full
featured threads, or POSIX ucontexts. A new data structure to store the shared
state of the system and synchronize the process execution was
implemented as well.

A new specific layer that acts as the OS kernel was implemented in SimGrid to
emulate systems calls, called \emph{requests}, and each time a user process
want to interact with other process, or the kernel itself, it raises
a \emph{request}.
After that, the engine takes control of the program and answer the
\emph{requests} of each process. This way the user processes can be parallelized
in a safe manner.

Experimental results showed that the new design does not hinder the tool
scalability. In fact, the sequential version of SimGrid remains orders of
magnitude more scalable than state of the art simulators.
The difficulty to get a parallel version of a P2P simulator faster than its
sequential counterpart was also revealed in ~\cite{previous}, being the first
time that a parallel simulation of Chord runs faster than the best known
sequential implementation.

An interesting result showed in the previous work is that the speedups only
increased up to a certain point when increasing the amount of working threads.
We also have proved that for small instances, parallelism actually hinders the
performance, and that the relative gain of parallelism seems even strictly
increasing with the system size.

Now we are closer to the optimal Amdahl's law threshold, that means that we have
reach a limit on the parallelizable portions of the code in our proposed model.
The remaining optimizations seek for a final speedup, trying to get a better
parallel threshold dynamically depending on the simulation, and better
performance of the threads taking in count their distribution on the CPU cores
and the different synchronization modes (futex, POSIX primitives or busy waiters).

#+LaTeX: \section{Performance Analysis}\label{sec:problem}
#+LaTeX: \subsection{Current speedup achieved} %Also, the benchmarking not intrusive is here.
We want to find the maximum speedup achieved with our current parallel
model. For that, a test a benchmark test is run to get the timings of a
typical sequential and parallel executions. After that, and using
the Amdahl's law, we can retrieve the real speedup achieved with
our system.

But first we want to prove that our benchmarks are not intrusive,
that is, our measures do not really affect the overall performance
of the system. For that, the experiments are run with and without
benchmarking, using the Precise mode, and then a comparation of
both is made to find if there is a significative breach in the
timings of both experiments.


#+name: benchnotintr
#+begin_src R :results output graphics :exports results :scale 1.8 :file bench-not-intrusive.pdf
orig_data = read.table("./optimizations_experiments/timings/total_times_noamdahl2.log")
opt_data = read.table("./optimizations_experiments/timings/total_sum_times_amdahl2.log")
orig_data = as.data.frame.matrix(orig_data)
opt_data = as.data.frame.matrix(opt_data)
#TODO: make experiments with 25000, 50000 and 75000
#Precise model, 2 threads (faster version with <10k nodes)
data <- data.frame(nodes =  orig_data[1:5,1], 
                   t2nobench = orig_data[1:5,9],
                   t2bench = opt_data[1:5,9],
		   t4nobench = orig_data[1:5,10],
		   t4bench = opt_data[1:5,10],
		   t16nobench = orig_data[1:5,12],
		   t16bench = opt_data[1:5,12],
                   t8nobench = orig_data[1:5,11],
                   t8bench = opt_data[1:5,11])
data[, "diff2"] <- abs(data$t2nobench - data$t2bench)
data[, "diff8"] <- abs(data$t8nobench - data$t8bench)
data[, "diff4"] <- abs(data$t4nobench - data$t4bench)
data[, "diff16"] <- abs(data$t16nobench - data$t16bench)

data[, "sum2"] <- data$t2nobench + data$t2bench
data[, "sum8"] <- data$t8nobench + data$t8bench
data[, "sum16"] <- data$t16nobench + data$t16bench
data[, "sum4"] <- data$t4nobench + data$t4bench

data[, "avg2"] <- data$sum2 / 2
data[, "avg8"] <- data$sum8 / 2
data[, "avg4"] <- data$sum4 / 2
data[, "avg16"] <- data$sum16 / 2

data[, "2 threads"] <- data$diff2 / data$avg2
data[, "16 threads"] <- data$diff16 / data$avg16
data[, "4 threads"] <- data$diff4 / data$avg4
data[, "8 threads"] <- data$diff8 / data$avg8


data[, "diff2"] <- NULL
data[, "diff8"] <- NULL
data[, "diff4"] <- NULL
data[, "diff16"] <- NULL
data[, "sum2"] <- NULL
data[, "sum8"] <- NULL
data[, "sum4"] <- NULL
data[, "sum16"] <- NULL
data[, "avg2"] <- NULL
data[, "avg8"] <- NULL
data[, "avg4"] <- NULL
data[, "avg16"] <- NULL
data[, "t2nobench"] <- NULL
data[, "t8nobench"] <- NULL
data[, "t4nobench"] <- NULL
data[, "t16nobench"] <- NULL
data[, "t2bench"] <- NULL
data[, "t8bench"] <- NULL
data[, "t4bench"] <- NULL
data[, "t16bench"] <- NULL
df <- melt(data ,  id = 'nodes', variable_name = 'difference')
g <- ggplot(df, aes(x=nodes,y=value, group=difference, color=difference)) + geom_line() + scale_fill_hue() + ylim(0,0.3)
plot(g)
#+end_src

#+attr_latex: width=0.1, height=0.1,placement=[p]
#+label: fig:one
#+caption: Percentage difference of time between benchmarked and original version.
#+results: benchnotintr
[[file:bench-not-intrusive.pdf]]

As it can be seen in the Figure \ref{fig:one}, the maximum difference in the execution
time of both versions is lesser than 15\%, and even lower with sizes bigger than 100000
nodes. That allow us to conclude that the benchmarking done to calculate the speedup is
not intrusive since it does not affect the parallel nor sequential executions of the simulation
in a significant way.

The experiment to calculate speedups involves the Chord simulation,
using the Precise model of our engine, and running it with 2,4,8,16 and 24 threads.
The actual speedup obtained can be seen in the Figure \ref{fig:two}.
It is clear from that graph that the real speedup with our parallel model is obtained
when the size of the problem is bigger than 100000 nodes.
This confirms what we have proved in ~\cite{previous}.

#+name: amdahl-speedup
#+begin_src R  :results output graphics :exports results :file amdahl-speedup.pdf
orig_data = read.table("./optimizations_experiments/timings/total_times_noamdahl2.log")
opt_data = read.table("./optimizations_experiments/timings/total_sum_times_amdahl2.log")
orig_data = as.data.frame.matrix(orig_data)
opt_data = as.data.frame.matrix(opt_data)
data <- data.frame(nodes =  orig_data[1:5,1], seq = opt_data[1:5,8], t2 = opt_data[1:5,9],
                   t4 = opt_data[1:5,10], t8 = opt_data[1:5,11], t16 = opt_data[1:5,12],
                   t24 = opt_data[1:5,13])
# an extra seq column to average would be good too.
data[, "speedup2"] <- data[, "seq"] / data[, "t2"]
data[, "speedup4"] <- data[, "seq"] / data[, "t4"]
data[, "speedup8"] <- data[, "seq"] / data[, "t8"]
data[, "speedup16"] <- data[, "seq"] / data[, "t16"]
data[, "speedup24"] <- data[, "seq"] / data[, "t24"]
data[, "base"] <- data[, "seq"] / data[, "seq"]


data[, "t2"] <- NULL
data[, "t4"] <- NULL
data[, "t8"] <- NULL
data[, "t16"] <- NULL
data[, "t24"] <- NULL
data[, "seq"] <- NULL

df <- melt(data ,  id = 'nodes', variable_name = 'versions')
ggplot(df, aes(x=nodes,y=value, group=versions, colour=versions)) + geom_line() #+ scale_colour_continuous(guide=FALSE)
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:two
#+caption: Real speedup achieved using parallell mode in Chord simulation.
#+results: amdahl-speedup
[[file:amdahl-speedup.pdf]]

#+LaTeX: \subsection{Parallelizable portions of the problem}
This experiment is based on a typical Chord simulation, and the data wanted
is the following: ID of each Scheduling Round, time taken by each Scheduling Round
and number of process executed in each scheduling round.

What we want to prove is that the limit on the speed up reached is due to the fact
that we are very closer to the line that define what is parallelizable in our model
and what is exeuted sequentially.
As it can be seen in the Figure \ref{fig:three} , the amount of processes computed by each scheduling
round is only one most of the times, so the parallel execution is not possible in that
instances. The remaining processes are executed in parallel due to the parallel
execution threshold already setted up in SimGrid (which can be modified), but it only
represents the 31\% of the total amount of user processes in a typical run.

#+name: sr-distribution
#+begin_src R  :results output graphics :exports results  :file sr-distribution.pdf

temp = list.files(path='./optimizations_experiments/sr_counts', pattern="*precise*", full.names = TRUE)
flist <- lapply(temp, read.table)
sr_data <- rbindlist(flist)
sr_data[, "V1"] <- NULL
sr_data = as.data.frame.matrix(sr_data)

ggplot(data=sr_data, geom="histogram", aes(x=V3)) + xlim(0,15) + geom_histogram(binwidth=0.5,aes(y=..count../sum(..count..))) + xlab("Amount of processes") + ylab("Percentage of Scheduling Rounds")
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:three
#+caption: Proportion of scheduling rounds computing processes.
#+results: sr-distribution
[[file:sr-distribution.pdf]]


Besides that, the Figure \ref{fig:four} show that when the amount of processes is biggger,
then the sequential execution time is bigger. That means that parallelizing that remaining 31\%
of processes is what gets makes the achieved speedup.

#+name: sr-times
#+begin_src R  :results output graphics :exports results  :file sr-times.pdf

temp = list.files(path='./optimizations_experiments/sr_counts/sequential', pattern="*precise*", full.names = TRUE)
flist <- lapply(temp, read.table)
sr_data <- rbindlist(flist)
sr_data[, "V1"] <- NULL
sr_data = as.data.frame.matrix(sr_data)
#for the mean
df <- ddply(sr_data, .(V3), summarize, mean_value = mean(V2))
ggplot(data=df, geom="histogram", aes(x=V3, y=mean_value)) + xlim(0,2000) + xlab("") + ylab("") + ylim(0,0.005) + geom_point(size = 1)
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:four
#+caption: Mean of times depending on the amount of processes of each scheduling round.
#+results: sr-times
[[file:sr-times.pdf]]


#+LaTeX: \section{Optimal threshold for parallel execution}
#+LaTeX: \subsection{Getting a real threshold over simulations}
The threshold wanted is how many processes are the right amount to be executed
in parallel when it is necessary, and when is it better to execute them in a
sequential way.
Initially, what we want is to find an optimal threshold for the beginning of
any simulation.
For that, a series of experiments have to be run using <version> of SimGrid.
That is why we test the performance of the engine in an exhaustive way,
benchmarking the scheduling rounds timings in parallel and sequential
executions, and finding the best average option for a simulation.

#+name: sr-par-threshold
#+begin_src R :results output graphics :exports results   :file sr-par-threshold.pdf
#SEQUENTIAL
temp = list.files(path='./optimizations_experiments/sr_counts/sequential2', pattern="*.log", full.names = TRUE)
flist <- lapply(temp, read.table)
sr_data <- rbindlist(flist) #TODO: SE PUEDE SACAR, CREO
sr_data[, "V1"] <- NULL
sr_data = as.data.frame.matrix(sr_data)
df <- ddply(sr_data, .(V3), summarize, mean_value = mean(V2))

#PARALLEL:
temp2 = list.files(path='./optimizations_experiments/sr_counts/parallel', pattern="*.log", full.names = TRUE)
flist2 <- lapply(temp2, read.table)
sr_data2 <- rbindlist(flist2) #TODO: SE PUEDE SACAR, CREO
sr_data2[, "V1"] <- NULL
sr_data2 = as.data.frame.matrix(sr_data2)
df2 <- ddply(sr_data2, .(V3), summarize, mean_value = mean(V2))

#merge this two datasets
df3 = merge(df, df2, by.x = 'V3', by.y = 'V3', incomparables = NULL)
df3[, 'speedup'] <- df3[,'mean_value.x'] / df3[, 'mean_value.y']
#for the mean
g <- ggplot(data=df3, geom="histogram", aes(x=V3, y=speedup)) + xlab("") + ylab("") +geom_line() + xlim(1,100)

#g <- ggplot(data=df3, geom="histogram", aes(x=V3, y=speedup)) + xlab("") + ylab("") +geom_line()
plot(g)
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[H]
#+label: fig:five
#+caption: Speedup obtained in relation with the amount of processes computed by scheduling round.
#+results:sr-par-threshold
[[file:sr-par-threshold.pdf]]

As it can be seen in the Figure \ref{fig:five}, despite being in a
constant or precise model, the speedup can be found starting from
45 user processes.

#+LaTeX: \section{Optimizations}\ref{sec:parallel}
#+LaTeX: \subsection{Binding threads to physical cores}
Talking about multicores CPUs, parallelization through threads is well
proved to be a good optimization, as we said in the previous section \ref{sec:problem}.
But there is still some improvements that can be done.

Thread execution depends heavily on the operative system scheduler: when one
thread is \emph{idle}, the scheduler may decide to switch it for another thread
ready to work, so it can maximize the occupancy of the cpu cores, and probably,
run a program in a faster way. Or it may just want to switch threads
because their execution time quote is over.

And when the first thread is ready to work again, the cpu core where it
was before might be occupied, forcing the system to run the thread in
another core. Of course this depend on which scheduler implementation we are using.

Regardless of the situation, migration of threads between cores entails an
increase of cache misses, which in a big size simulation can be detrimental to
the performance.

Moreover, in a dedicated computer with many cores (like a cluster node, for example),
thread migration between the cores might be senseless and might introduce unnecessary
cache misses.

This is actually our case: the amount of cache misses is XX% in a typical Chord simulation
while with threads binding it is reduced until XX%.
TODO: a quick study of cache misses using perf with small amount nodes. Just
write the percentage, dont plot/table nothing

In order to avoid these cache misses produced by regular context switching
of threads, Glib offers a way to bind each thread to a physical core
of the CPU. Note that this is only available in Linux platforms.

A Chord simulation was run in a parapluie node, with 24 cores, and the speedup gained
binding the threads to cores was XXX in the best case, as it can be seen in the
Figure \ref{fig:six}.

#+name: binding-speedup
#+begin_src R  :results output graphics :exports results :file binding-speedup.pdf
orig_data = read.table("./optimizations_experiments/timings/total_times_noamdahl2.log")
opt_data = read.table("./optimizations_experiments/binding_cores/timings_64be642.dat")
orig_data = as.data.frame.matrix(orig_data)
opt_data = as.data.frame.matrix(opt_data)
#Legend: t2=time with binding; t2nb = time with No Binding
data <- data.frame(nodes =  orig_data[1:5,1], t2nb = orig_data[1:5,3], t4nb = orig_data[1:5,4],
                   t8nb = orig_data[1:5,5], t16nb = orig_data[1:5,6], t24nb = orig_data[1:5,7],
                   t2 = opt_data[1:5,9],
                   t4 = opt_data[1:5,10], t8 = opt_data[1:5,11], t16 = opt_data[1:5,12],
                   t24 = opt_data[1:5,13])

data[, "speedup2"] <- data[, "t2nb"] / data[, "t2"]
data[, "speedup4"] <- data[, "t4nb"] / data[, "t4"]
data[, "speedup8"] <- data[, "t8nb"] / data[, "t8"]
data[, "speedup16"] <- data[, "t16nb"] / data[, "t16"]
data[, "speedup24"] <- data[, "t24nb"] / data[, "t24"]
data[, "base"] <- data[, "t2"] / data[, "t2"]
data[, "t2"] <- NULL
data[, "t4"] <- NULL
data[, "t8"] <- NULL
data[, "t16"] <- NULL
data[, "t24"] <- NULL
data[, "t2nb"] <- NULL
data[, "t4nb"] <- NULL
data[, "t8nb"] <- NULL
data[, "t16nb"] <- NULL
data[, "t24nb"] <- NULL

df <- melt(data ,  id = 'nodes', variable_name = 'versions')
ggplot(df, aes(x=nodes,y=value, group=versions, colour=versions)) + geom_line() #+ scale_colour_continuous(guide=FALSE)
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:two
#+caption: Speedup achieved binding threads to physical CPU cores.
#+results: binding-speedup
[[file:binding-speedup.pdf]]



#+LaTeX: \subsection{Parmap between N cores}

#+LaTeX: \subsection{Busy Waiters}

#+LaTeX: \subsection{Performance Regression Testing}

#+LaTeX: \subsection{Adaptive algorithm to calculate threshold}
Finding an optimal threshold and keep it during all the simulation might not
always be the best option: some simulations can take more or less time in
the execution of user processes. If a simulation has
very efficient processes, or processes that don't work too much, then the
threshold could be inapropiate, leading to parallelize scheduling rounds
that would run more efficiently in a sequential way.
That's why an algorithm for a dynamic threshold calculation is proposed.


TODO: explanation of the heuristic...bla bla is the amount of time taken by
each scheduling round, and calculate on the fly a dynamic threshold to fit
better the simulation. Pseudocode



: Models used in the chord simulations
: \begin{itemize}
: \item Workstation model: Default vm workstation model (as it appears on ./chord --help)
: \item Network Model: LV08 (or Constant)
: \item Cpu Model: Cas01
: \end{itemize}

#+name: adapt-algorithm
#+begin_src R  :results output graphics :exports results  :file adapt-algorithm.pdf

orig_data = read.table("./optimizations_experiments/dynamic_threshold/optimization3.dat")
opt_data = read.table("./optimizations_experiments/dynamic_threshold/optimization3_part2.dat")
orig_data = as.data.frame.matrix(orig_data)
opt_data = as.data.frame.matrix(opt_data)
#data <- data.frame(nodes =  orig_data[1:4,1], thr4const=orig_data[1:4,2], thr8const=orig_data[1:4,3], thr16const=orig_data[1:4,4], thr4prec=orig_data[1:4,5],thr8prec=orig_data[1:4,6],thr16prec=orig_data[1:4,7],optthr4const=opt_data[1:4,2], optthr8const=opt_data[1:4,3], optthr16const=opt_data[1:4,4], optthr4prec=opt_data[1:4,5], optthr8prec=opt_data[1:4,6],optthr16prec=opt_data[1:4,7])
data <- data.frame(nodes =  orig_data[1:4,1], thr4prec=orig_data[1:4,5],thr8prec=orig_data[1:4,6],thr16prec=orig_data[1:4,7],optthr4prec=opt_data[1:4,5], optthr8prec=opt_data[1:4,6],optthr16prec=opt_data[1:4,7])
df <- melt(data ,  id = 'nodes', variable_name = 'versions')
ggplot(df, aes(x=nodes,y=value, group=versions, colour=versions)) + geom_line() + scale_fill_hue()
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:seven
#+caption: Chord simulation, Precise Model. Original version vs. Adaptative algorithm.
#+results: adapt-algorithm
[[file:adapt-algorithm.pdf]]


#+LaTeX: \section{Conclusion}\label{sec:cc}

#+LaTeX: \section{References}\label{sec:ref}

#+LaTeX: \end{document}
