
#+TITLE: Parallel and Distributed Simulation of Large-Scale Distributed Applications
#+AUTHOR:  Ezequiel Torti LÃ²pez, Martin Quinson
#+OPTIONS: H:5 title:nil date:nil author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil
#+STARTUP: indent hideblocks
#+TAGS: noexport(n)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+PROPERTY: session *R* 

#+LATEX_class: sigalt
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{ifthen,figlatex}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage[american]{babel}
#+LATEX_HEADER: \usepackage{url}\urlstyle{sf}
#+LATEX_HEADER: \usepackage{amscd}
#+LATEX_HEADER: \usepackage{wrapfig}

#+begin_src R  :exports none
# If you miss the libraries, try typing >>>install.packages("data.table")<<< in a R console
library('ggplot2')
library('gridExtra')
library('reshape')
library('plyr')
library('data.table')
library('stringr')
require('grid')
# To plot several ggplot in one window.
vp.layout <- function(x, y) viewport(layout.pos.row=x, layout.pos.col=y)
arrange_ggplot2 <- function(..., nrow=NULL, ncol=NULL, as.table=FALSE) {
    dots <- list(...)
    n <- length(dots)
    if(is.null(nrow) & is.null(ncol)){
        nrow = floor(n/2) ; ncol = ceiling(n/nrow)
    }
    if(is.null(nrow)){
        nrow = ceiling(n/ncol)
    }
    if(is.null(ncol)){
        ncol = ceiling(n/nrow)
    }
    grid.newpage()
    pushViewport(viewport(layout=grid.layout(nrow,ncol)))
    ii.p <- 1
    for(ii.row in seq(1, nrow)){
        ii.table.row <- ii.row
        if(as.table) {
            ii.table.row <- nrow - ii.table.row + 1
        }
        for(ii.col in seq(1, ncol)){
            ii.table <- ii.p
            if(ii.p > n) break
            print(dots[[ii.table]], vp=vp.layout(ii.table.row, ii.col))
            ii.p <- ii.p + 1
        }
    }
}

# Get legend from a given plot
g_legend<-function(a.gplot){
    tmp <- ggplot_gtable(ggplot_build(a.gplot))
    leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
    legend <- tmp$grobs[[leg]]
    return(legend)
}
#+end_src

#+RESULTS:

* Motivation and Problem Statement

Simulation is the third pillar of science, allowing to study complicated
phenomenons through complex models. When the size or complexity of the studied
models becomes too large, it is classical to leverage more resources through
Parallel Discrete-Event Simulation (PDES).

Still, the parallel simulation of very fine grained applications deployed on
large-scale distributed systems (LSDS) remains challenging. As a matter of fact,
most simulators of Peer-to-Peer systems are sequential, despite the vast
literature on PDES over the last three decades.

dPeerSim is one of the very few existing PDES for P2P systems, but it presents
deceiving performance: it can achieve a decent speedup when increasing the
amount of logical processes (LP): from 4h with 2 LPs down to 1h with 16 LPs.
But it remains vastly inefficient when compared to sequential version of
PeerSim, that performs the same experiment in 50 seconds only. This calls for a
new parallel schema specifically tailored to this category of Discrete Event
Simulators.

Discrete Event Simulation of Distributed Applications classically alternates
between simulation phases where the models compute the next event date, and
phases where the application workload is executed.  We proposed

in~\cite{previous} to not split the simulation model across several computing
nodes, but instead to keep the model sequential and execute the application
workload in parallel when possible. We hypothesized that this would help
reducing the synchronization costs. We evaluate our contribution with very fine
grained workloads such as P2P protocols. These workloads are the most difficult
to execute efficiently in parallel because execution times are very short,
making it very difficult to amortize the synchronization times.

We implemented this parallel schema within the SimGrid framework, and showed
that the extra complexity does not endangers the performance since the
sequential version of SimGrid still outperforms several competing solutions when
our addition are present but disabled at run time.

To the best of our knowledge, it is the first time that a parallel simulation of
P2P system proves to be faster that the best known sequential execution. Yet,
the parallel simulation only outperforms sequential one when the amount of
processes becomes large enough. This is because of the pigonhole principle: when
the amount of processes increases, the average amount of processes that are
ready to run at each simulated timestamp (and can thus run in parallel)
increases. When simulating the Chord protocol, it takes 500,000 processes or
more to amortizing the synchronization costs, while the classical studies of the
literature usually involve less processes.

The current work aims at further improving the performance of our PDES, using
several P2P protocols as a workload. We investigate the possible inefficiency
and propose generic solutions that could be included in other similar simulators
of large-scale distributed systems, be them P2P simulators of cloud, HPC or
sensornets ones.

This paper is organized as follows: Section \ref{sec:context} recaps the SimGrid
architecture and quickly presents the parallel execution schema detailed in
\cite{previous}. Section \ref{sec:problem} analysis the theoretical performance
bound, and discusses the previous work at the light of the Amhdal law. Section
\ref{sec:parallel} explores several trade-offs for the efficiency of the
parallel sections. Section \ref{sec:adaptive} proposes an algorithm to
automatically tune the level of parallelism that is adapted to the simulated
application. Section \ref{sec:cc} concludes this paper and discusses some future
work.

* Context
#+LaTeX: \label{sec:context}

In the previous work ~\cite{previous} we proposed to parallelize the execution
of the user code while keeping the simulation engine sequential.  This is
enabled by applying classical concepts of OS design to this new context: every
interaction between the user processes (from now on, user processes and
processes mean the same thing) and the simulated environment passes through a
specific layer that act as an OS kernel.

A novel way to virtualize user processes (\emph{raw contexts}) was crafted to
improve efficiency and avoid unnecesary system calls, but other ways to do this
can be found for the sake of portability, such as full featured threads, or
POSIX ucontexts. A new data structure to store the shared state of the system
and synchronize the process execution was implemented as well.

A new specific layer that acts as the OS kernel was implemented in SimGrid to
emulate systems calls, called \emph{requests}, and each time a user process want
to interact with other process, or the kernel itself, it raises a
\emph{request}.  After that, the engine takes control of the program and answer
the \emph{requests} of each process. This way the user processes can be
parallelized in a safe manner.

Experimental results showed that the new design does not hinder the tool
scalability. In fact, the sequential version of SimGrid remains orders of
magnitude more scalable than state of the art simulators.  The difficulty to get
a parallel version of a P2P simulator faster than its sequential counterpart was
also revealed in ~\cite{previous}, being the first time that a parallel
simulation of Chord runs faster than the best known sequential implementation.

An interesting result showed in the previous work is that the speedups only
increased up to a certain point when increasing the amount of working threads.
We also have proved that for small instances, parallelism actually hinders the
performance, and that the relative gain of parallelism seems even strictly
increasing with the system size.

Now we are closer to the optimal Amdahl's law threshold, that means that we have
reach a limit on the parallelizable portions of the code in our proposed model.
The remaining optimizations seek for a final speedup, trying to get a better
parallel threshold dynamically depending on the simulation, and better
performance of the threads taking in count their distribution on the CPU cores
and the different synchronization modes (futex, POSIX primitives or busy
waiters).

* Performance Analysis
#+LaTeX: \label{sec:problem}
** Current speedup achieved
# Also, the benchmarking not intrusive is here.
We want to find the maximum speedup achieved with our current parallel
model. For that, a benchmark test is run to get the timings of typical
sequential and parallel executions, so we can get the real speedup achieved with
our system using the Amdahl's law.

But first we want to prove that our benchmarks are not intrusive, that is, our
measures do not really affect the overall performance of the system. For that,
the experiments are run with and without benchmarking, using the Precise mode,
and then a comparation of both is made to find if there is a significative
breach in the timings of both experiments.

Using the Chord simulation, the experiment showed us that the maximum difference
in the execution time of both versions is lesser than 10% in most of the cases,
and is even lower with sizes bigger that 100000 nodes, which allow us to
conclude that the benchmarking is, indeed, not intrusive.

The experiment to calculate speedups involves the Chord simulation as well,
using the Precise model of our engine, and running it with 2,4,8,16 and 24
threads.  The actual speedup obtained can be seen in the Figure \ref{fig:one}.
It is clear from that graph that the real speedup with our parallel model is
obtained when the size of the problem is bigger than 100000 nodes.  This
confirms what we have proved in ~\cite{previous}.

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:one
#+caption: Real speedup achieved running Chord simulation in multithread mode.
#+results: amdahl-speedup
[[file:fig/amdahl-speedup.pdf]]

** Parallelizable portions of the problem
This experiment is based on a typical Chord simulation, and the data wanted is
the following: ID of each Scheduling Round, time taken by each Scheduling Round
and number of process executed in each scheduling round.

What we want to prove is that the limit on the speed up reached is due to the
fact that we are very closer to the line that define what is parallelizable in
our model and what is exeuted sequentially.  As it can be seen in the Figure
\ref{fig:two} , the amount of processes computed by each scheduling round is
only one most of the times, so the parallel execution is not possible in that
instances. The remaining processes are executed in parallel due to the parallel
execution threshold already setted up in SimGrid (which can be modified), but it
only represents the 31\% of the total amount of user processes in a typical run.

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:two
#+caption: Proportion of scheduling rounds computing processes depending on amount of nodes simulated (Chord simulation).
#+results: sr-distribution
[[file:fig/sr-distribution.pdf]]


Besides that, the Figure \ref{fig:three} show that when the amount of processes is bigger,
then the sequential execution time is bigger. That means that parallelizing that remaining 31\%
of processes is what achieves the current speedup.

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:three
#+caption: Execution times of SR's depending on the amount of processes of each SR (Chord simulation)
#+results: sr-times
[[file:fig/sr-times.pdf]]


* Optimizations
#+LaTeX: \label{sec:parallel}
** Binding threads to physical cores
Regarding the multicore architectures (like almost every modern CPU),
parallelization through threads is well proved to be a good optimization, as we
said in the previous section \ref{sec:problem}. But there are still some
improvements that can be done.

Thread execution depends heavily on the operative system scheduler: when one
thread is \emph{idle}, the scheduler may decide to switch it for another thread
ready to work, so it can maximize the occupancy of the cpu cores, and probably,
run a program in a faster way. Or it may just want to switch threads because
their execution time quote is over.

And when the first thread is ready to work again, the cpu core where it was
before might be occupied, forcing the system to run the thread in another
core. Of course this depend on which scheduler implementation we are using.

Regardless of the situation, migration of threads between cores entails an
increase of cache misses, and the amount of CPU migrations in a big size
simulation can be detrimental for the performance.

In order to avoid these CPU migrations produced by a constant context switching
of threads, Glib offers a way to bind each thread to a physical core of the
CPU. Note that this is only available in Linux platforms.

A Chord simulation was run in a parapluie node, with 24 cores, binding the
threads to physical cores. The CPU migration was drastically reduced (almost
97\% less migrations) in all the cases.  The speedup obtained with few threads
(2, 4 and 8) was not big enough: x1.63 in the best case, and x1.23 in average.
But when the simulation is run with a bigger amount of threads (16 or 24), the
impact of having less CPU migrations is notable, being obtained speedups between
x2.44 and almost x15 (depending on the amount of threads and the size of the
simulation).  This proves that physical binding of threads to CPU cores can be
useful when a big amount of threads is needed.


** Parmap between N cores

Several optimizations regarding the distribution of work between threads were
proposed: the first option is the default one, where maestro works with its
threads and the processes are distributed equitably between each thread; the
second one is to send maestro to sleep and let the worker threads do all the
computing; the last one involves the creation of one extra thread and make all
this N threads work while maestro sleeps.

The experiments were made using up to 75000 nodes in a Chord simulation with
Precise and Constant modes, but no performance gain was achieved. In fact, the
creation of one extra thread proved to be slightly more slower than the original
version of parmap, while sending maestro to sleep and make its N-1 threads do
the computation did not show any improvement or loss in performance.

** Busy Waiting versus Futexes
SimGrid provides several types of synchronization between threads: Fast
Userspace Mutex (futex), the classical POSIX synchronization primitives and busy
waiters.  While each of them can be chosen when running the simulation, futexes
are the default option, since they have the advantage to implement a fast
synchronization mode within the parmap abstraction, in user space only.  But
even when they are more efficient than classical mutexes, which run in kernel
space, they may present performance drawbacks.  In this section we compare the
busy waiters vs. futexes synchronization types, using the chord simulation in
both Constant and Precise modes, and we found that using busy waiters instead of
futexes can result in a speedup between 1.08 and 1.53 using few threads (up to
8), and from 1.15 to 3 when the amount of threads is big enough (16 or 24).

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:four
#+caption: Relative speedup of busy waiters vs. futexes in Chord simluation.
#+results:busy
[[file:fig/busy.pdf]]

** Performance Regression Testing

* Optimal threshold for parallel execution
#+LaTeX: \label{sec:adaptive}
** Getting a real threshold over simulations
The threshold wanted is how many processes are the right amount to be executed
in parallel when it is necessary, and when is it better to execute them in a
sequential way.  Initially, what we want is to find an optimal threshold for the
beginning of any simulation.  For that purpose, we have done a benchmark of the
scheduling rounds timings in parallel and sequential executions, and have found
the best average option for a simulation.

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:five
#+caption: Speedup of parallel vs. sequential executions of SR's, depending in the number of processes taken by each SR
#+results:sr-par-threshold
[[file:fig/sr-par-threshold.pdf]]

As it can be seen in the Figure \ref{fig:five}, the optimal threshold starts at 28 user processes
for the Precise mode, while with Constant mode, the speedup of parallel vs. sequential execution of
scheduling rounds is immediate.

** Adaptive algorithm to calculate threshold
Finding an optimal threshold and keep it during all the simulation might not
always be the best option: some simulations can take more or less time in the
execution of user processes. If a simulation has very efficient processes, or
processes that don't work too much, then the threshold could be inappropriate,
leading to parallelize scheduling rounds that would run more efficiently in a
sequential way.  That's why an algorithm for a dynamic threshold calculation is
proposed.

The main idea behind this heuristic is to calculate the optimal number of
processes that can be run in parallel during the execution of the simulation.

For that purpose, the times of five scheduling round are measured.  A
performance ratio for both of the possible parallel and sequential executions is
calculated, simply by dividing the time taken by the amount of processes
computed.  If the sequential ratio turns to be bigger than the parallel one,
then the threshold is decreased, and increased otherwise.

This results in a relative improvement in performance. As it can be seen on
Figure \ref{fig:five}, the speedup is important in the 16 and 24 threads cases,
reaching levels between 11 and 47 with small sizes (1000, 5000, and 10000
nodes), while with fewer amounts of threads (2,4,8) the speedup is not big,
between 1.28 and 4.62.  This can be explained by the fact that the algorithm
choose more efficiently when to launch a scheduling round in parallel, and while
having a lot of threads increases the costs of synchronization, an intelligent
choice of when to launch them will reduce that cost.


#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:six
#+caption: Speedup achieved with Adaptive Algorithm. Chord simulation, Precise and Constant modes.
#+results: adapt-algorithm
[[file:fig/adapt-algorithm.pdf]]


* Conclusion
#+LaTeX: \label{sec:cc}


#+LaTeX: \appendix
* Data Provenance
This section explains and show how to run the experiments and how the data is
saved and then processed.  Note: that all experiments are run using the Chord
simulation that can be found in examples/msg/chord folder of your SimGrid
install. Unless stated, all the experiments are run using the futex
synchronization method and raw contexts under a Linux environment; in a node
from 'parapluie' cluster of Grid5000.  The analysis of data can be done within
this paper itself, executing the corresponding R codes. Note that it is even
possible to execute them remotely if TRAMP is used to open this file (this is
useful if you want the data to be processed in one powerful machine, such as a
cluster).
** Setting up a node in Grid5000
This section is useful only if you have an account in Grid5000 and want to run the
experiments there.
Once you have a node deployed, you can do something similar to what is in the
./scripts/g5k/setup_node.sh script to install packages/compile SimGrid.
*** DONE make setup_node.sh more portable to other users.
** Amdahl speedup
The constant TIME_BENCH_AMDAHL must be defined in SimGrid in order to
enable the required logs for this experiment. This variable can be
defined in the file src/simix/smx_private.h
The experiments were done using the Chord simulation that can be found in the
SimGrid folder /examples/msg/chord.
To run the experiment, simply run the script "testall.sh" inside the folder
examples/msg/chord of your SimGrid install. The script can be found in the
folder ./scripts/chord of this repository.
Inside that script, the number of threads to test, as well as the amount
of nodes, can be modified

The logs gathered must be processed with get_times.py,
located in the same folder as testall.sh. This generates a .csv that
can easily be plotted with R/gnuplot. Note that the script is self-documented.
** SR Distribution
To enable Scheduling Rounds benchmarks, the constant TIME_BENCH_PER_SR
has to be defined. It can be defined in src/simix/smx_private.h
The logs give information about the time it takes to run a scheduling
round, as well as the amount of processes each SR takes.
For this experiment, we are only interested in the amount of processes
taken by each SR.

The script to run this experiment is ./scripts/chord/testall_sr.sh, it
is mandatory to have also the get_sr_counts.py scripts to process output
data and save only the id of SR, time of SR and num processes of SR,
in a table format.

To run, simply put testall_sr.sh and get_sr_counts.py in the examples/msg/chord
folder of your SimGrid, and run ./testall_sr.sh
*** DONE put the R code of post-processing the tables in the Data Analysis section (separated from the plot generation code)
*** DONE this plot has to be changed for a more representative one
** SR Times
The data set used for this plot is the same as the one before.
We just use the data of the sequential simulations (1 thread).
** Binding threads to physical cores
The constant CORE_BINDING has to be defined in src/xbt/parmap.c in
order to enable this optimization.
The benchmark is then run in the same way as the Amdahl Speedup experiment
(using the same script in the same way) and then creating a table with the
times using the scripts/chord/get_times.py script
** parmap between N cores
This may be the experiment that requires more work to reproduce:
*** maestro works with N-1 threads
This is the default setting. Just run the testall.sh, get the logs and
create a table using get_times.py
*** maestro sleeps with N-1 threads
To avoid that maestro works with the threads, just comment out the line:
    xbt_parmap_work(parmap);
from the function xbt_parmap_apply() in src/xbt/parmap.c

Then just run the testall.sh benchmark the same as before. Create tables
with get_times.py
*** maestro sleeps with N threads
This requires to do the same as before. Then the function
src/xbt/parmap.c:xbt_parmap_new has to be modified to create
one extra thread. It is easy: just add 1 to num_workers parameter.

Then just run testall.sh and then get_times.py the same as before.
** Busy Waiters vs. Futexes performance
Enable the use of busy waiters running chord with the extra option:
    --cfg=contexts/synchro:busy_wait
The experiment was run with testall.sh using that extra option in the
chord command inside the script. The tables were constructed using get_times.py
The data regarding the futexes synchro times is the same gathered in Amdahl
Speedup experiment.
Then both tables can be compared in a plot.
** Performance Regression Testing
*** TODO not yet...
** SR parallel threshold
The data set is the same as SR Distribution and SR times experiments.
*** DONE put code to post-process tables in Data Analysis, separated from plot code
** Adaptive Algorithm
The benchmark is done using testall.sh and the tables generated using get_times.py
* Data Analysis                                                    :noexport: 
** Pre-processing of datasets
#+name: process_data_sr-times
#+begin_src R
temp = list.files(path='./logs/sr_counts/sequential', pattern="*precise*", full.names = TRUE)
flist <- lapply(temp, read.table)
sr_data <- rbindlist(flist)
sr_data[, "V1"] <- NULL
sr_data = as.data.frame.matrix(sr_data)
saveRDS(sr_data, file="./logs/sr_counts/sr-times.Rda")
#+end_src

#+name: process_data_sr-par-threshold
#+begin_src R
#PRECISE MODE
#SEQUENTIAL
temp = list.files(path='./logs/sr_counts/sequential', pattern="threads4_", full.names = TRUE)
temp <- temp[grepl("precise", temp)]
#temp <- temp[-grep("50000", temp)]
#temp <- temp[-grep("75000", temp)]
flist <- lapply(temp, read.table)
sr_data <- rbindlist(flist)
sr_data[, "V1"] <- NULL
sr_data = as.data.frame.matrix(sr_data)
df <- ddply(sr_data, .(V3), summarize, mean_value = mean(V2))

#PARALLEL:
temp2 = list.files(path='./logs/sr_counts/parallel', pattern="threads4_", full.names = TRUE)
temp2 <- temp2[grepl("precise", temp2)]
flist2 <- lapply(temp2, read.table)
sr_data2 <- rbindlist(flist2)
sr_data2[, "V1"] <- NULL
sr_data2 = as.data.frame.matrix(sr_data2)
df2 <- ddply(sr_data2, .(V3), summarize, mean_value = mean(V2))

#CONSTANT MODE
#SEQUENTIAL
temp3 = list.files(path='./logs/sr_counts/sequential', pattern="threads4_", full.names = TRUE)
temp3 <- temp3[grepl("constant", temp3)]
flist <- lapply(temp3, read.table)
sr_data <- rbindlist(flist)
sr_data[, "V1"] <- NULL
sr_data = as.data.frame.matrix(sr_data)
df3 <- ddply(sr_data, .(V3), summarize, mean_value = mean(V2))


#PARALLEL:
temp4 = list.files(path='./logs/sr_counts/parallel', pattern="threads4_", full.names = TRUE)
temp4 <- temp4[grepl("constant", temp4)]
#temp4 <- temp4[-grep("50000", temp4)]
#temp4 <- temp4[-grep("75000", temp4)]
flist2 <- lapply(temp4, read.table)
sr_data2 <- rbindlist(flist2)
sr_data2[, "V1"] <- NULL
sr_data2 = as.data.frame.matrix(sr_data2)
df4 <- ddply(sr_data2, .(V3), summarize, mean_value = mean(V2))

#Merge PRECISE datasets
df5 = merge(df, df2, by.x = 'V3', by.y = 'V3', incomparables = NULL)
df5[, 'speedup'] <- df5[,'mean_value.x'] / df5[, 'mean_value.y']
saveRDS(df5, file="./logs/sr_counts/precise.Rda")
#Merge CONSTANT datasets
df6 = merge(df3, df4, by.x = 'V3', by.y = 'V3', incomparables = NULL)
df6[, 'speedup'] <- df6[,'mean_value.x'] / df6[, 'mean_value.y']
saveRDS(df6,file="./logs/sr_counts/constant.Rda")
#+end_src
** Plotting
#+name: amdahl-speedup
#+begin_src R  :results output graphics :exports results :file fig/amdahl-speedup.pdf
data = read.csv("./logs/timings/total_sum_times_amdahl.csv", head=TRUE, sep=',')
# Speedups of Constant Mode
data[, "sc1"]  <- data[, "c1"]  / data[, "c1"]
data[, "sc2"]  <- data[, "c1"] / data[, "c2"]
data[, "sc4"]  <- data[, "c1"] / data[, "c4"]
data[, "sc8"]  <- data[, "c1"] / data[, "c8"]
data[, "sc16"] <- data[, "c1"] / data[, "c16"]
keep <- c("nodes", colnames(data)[grep("^sc", colnames(data))])
speedup_constant <- data[keep]
df <- melt(speedup_constant ,  id = 'nodes', variable_name = 'threads')
g1<-ggplot(df, aes(x=nodes,y=value, group=threads, colour=threads)) + geom_line() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),
    panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(), axis.title.x = element_blank(),
    legend.position="none") + ylab("Constant mode")

# Speedups of Precise Mode
data[, "1"] <- data[, "p1"]  / data[, "p1"]
data[, "2"] <- data[, "p1"]  / data[, "p2"]
data[, "4"] <- data[, "p1"]  / data[, "p4"]
data[, "8"] <- data[, "p1"]  / data[, "p8"]
data[, "16"] <- data[,"p1"]  / data[, "p16"]
keep <- c("nodes", colnames(data)[grep("^[1-9]", colnames(data))])
speedup_precise <- data[keep]
df2 <- melt(speedup_precise ,  id = 'nodes', variable_name = 'threads')
g2<-ggplot(df2, aes(x=nodes,y=value, group=threads, colour=threads)) + geom_line() + theme(axis.text.x = element_text(angle = -45, hjust = 0),
    panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(),
    axis.title.x = element_blank(), legend.position="right") + ylab("Precise mode") + scale_fill_discrete(name="threads")

legend <- g_legend(g2)
lwidth <- sum(legend$width)

grid.arrange(arrangeGrob(g1, g2 + theme(legend.position="none"),
             main ="",sub="Amount of nodes simulated",
             left = "Speedups"),legend, 
             widths=unit.c(unit(1, "npc") - lwidth, lwidth), nrow=1)
#+end_src

#+name: sr-distribution
#+begin_src R
temp = list.files(path='./logs/sr_counts/sequential', pattern="threads4_", full.names = TRUE)
temp <- temp[grep("precise",temp)]
# This data.frame will store the final proportion values.
proportions <- data.frame(stringsAsFactors=FALSE) 
for(i in temp){
    # Parse amount of nodes from the file path.
    # Example of file path: './logs/sr_counts/parallel/sr_10000_threads4_constant.log'
    nodes = as.numeric(strsplit(str_extract(i, "_[0-9]+_"), "_")[[1]][2])
    # Keep only the column with the amount of processes
    data <- read.table(i)["V3"]
    # Calculate proportions
    data <- prop.table(xtabs(~ V3, data=data))
    # Populate a new data frame with percentages of interest (1, 2, 3 or more processes)
    p <- c(data["1"][[1]],data["2"][[1]], 1 - (data["1"][[1]] + data["2"][[1]]))
    # And bind to existing data.frame
    p <- as.data.frame(p)
    p[,'nodes'] <- nodes
    p[,'process'] <- c("1","2",">3")
    proportions <- rbind(proportions, p)
}
df <- ddply(proportions, .(nodes,process), summarise, msteps = mean(p))
g<-ggplot(df, aes(x=nodes,y=msteps, group=process, colour=process)) + geom_line() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.title.x = element_text("Nodes simulated"), axis.title.y = element_text("Percentage of SR"), axis.line=element_line()) + scale_fill_discrete(name="threads")
g
#+end_src

#+name: sr-times
#+begin_src R  :results output graphics :exports results  :file fig/sr-times.pdf
sr_data <- readRDS(file="./logs/sr_counts/sr-times.Rda")
#df <- ddply(sr_data, .(V3), summarize, mean_value = mean(V2))
ggplot(data=sr_data, geom="histogram", aes(x=V3, y=V2)) + xlim(0,500) + xlab("Processes in SR") + ylab("Time consumed to compute SR's") + ylim(0,0.005) + geom_point(size = 1) # Replace V2 for 'mean_value' if dont want to plot every dot; and uncomment line above.
#+end_src

#+name: busy
#+begin_src R :results output graphics :exports results :file fig/busy.pdf
orig_data = read.csv("./logs/busy_waiters/total_times_orig.csv", head=TRUE, sep=',')
opt_data = read.csv("./logs/busy_waiters/total_times_busy.csv", head=TRUE, sep=',')

# Speedups of Constant Mode
opt_data[, "1"]  <- orig_data[, "c2"]  / orig_data[, "c2"]
opt_data[, "2"]  <- orig_data[, "c2"]  / opt_data[, "c2"]
opt_data[, "4"]  <- orig_data[, "c4"]  / opt_data[, "c4"]
opt_data[, "8"]  <- orig_data[, "c8"]  / opt_data[, "c8"]
opt_data[, "16"] <- orig_data[, "c16"] / opt_data[, "c16"]
opt_data[, "24"] <- orig_data[, "c24"] / opt_data[, "c24"]
keep <- c("nodes", colnames(opt_data)[grep("^[1-9]", colnames(opt_data))])
speedup_constant <- opt_data[keep]

# Speedups of Precise Mode
opt_data[, "1"]  <- orig_data[, "p2"]  / orig_data[, "p2"]
opt_data[, "2"]  <- orig_data[, "p2"]  / opt_data[, "p2"]
opt_data[, "4"]  <- orig_data[, "p4"]  / opt_data[, "p4"]
opt_data[, "8"]  <- orig_data[, "p8"]  / opt_data[, "p8"]
opt_data[, "16"] <- orig_data[, "p16"] / opt_data[, "p16"]
opt_data[, "24"] <- orig_data[, "p24"] / opt_data[, "p24"]
keep <- c("nodes", colnames(opt_data)[grep("^[1-9]", colnames(opt_data))])
speedup_precise <- opt_data[keep]

df <- melt(speedup_constant,  id = 'nodes', variable_name = 'threads')
df2 <- melt(speedup_precise ,  id = 'nodes', variable_name = 'threads')

g2<-ggplot(df2, aes(x=nodes,y=value, group=threads, colour=threads)) + geom_line() +
    scale_fill_hue() + theme(axis.text.x = element_text(angle = -45, hjust = 0),
    panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(),
    axis.title.x = element_blank(), legend.position="right") + 
    scale_x_continuous(breaks=c(1000,5000,10000,25000,50000,75000,100000)) +
    scale_y_continuous(breaks=c(1.0,1.5,2.0,2.5,3.0,4.0)) + ylab("Precise mode")

g1<-ggplot(df, aes(x=nodes,y=value, group=threads, colour=threads))  + geom_line() +
    scale_fill_hue() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),
    panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(), axis.title.x = element_blank(),
    legend.position="none") +
    scale_x_continuous(breaks=c(1000,5000,10000,25000,50000,75000,100000)) +
    scale_y_continuous(breaks=c(1.0,1.5,2.0,2.5,3.0,4.0)) + ylab("Constant mode")

legend <- g_legend(g2)
lwidth <- sum(legend$width)

grid.arrange(arrangeGrob(g1, g2 + theme(legend.position="none"),
             main ="",sub="Nodes",
             left = "Speedups"),legend, 
             widths=unit.c(unit(1, "npc") - lwidth, lwidth), nrow=1)
#+end_src

#+name: sr-par-threshold
#+begin_src R :results output graphics :exports results   :file fig/sr-par-threshold.pdf
precise <- readRDS(file="./logs/sr_counts/precise.Rda")
constant <- readRDS(file="./logs/sr_counts/constant.Rda")

g1<- ggplot(data=constant, geom="histogram", aes(x=V3, y=speedup)) + xlab("")+geom_point() + xlim(1,100) + stat_smooth(se=FALSE) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),
    panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(), axis.title.x = element_blank(),
    legend.position="none") + ylab("Constant mode")

g2<-ggplot(data=precise, geom="histogram", aes(x=V3, y=speedup)) + xlab("Processes in SR") +geom_point() + xlim(1,100) + stat_smooth(se=FALSE) + theme(panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(),
    axis.title.x = element_blank(), legend.position="none") + ylab("Precise mode")

grid.arrange(arrangeGrob(g1, g2,
             main ="", sub = "Amount of processes taken by SR's",
             left = "Speedup of parallel execution against sequential execution"), nrow=1)
#+end_src

#+name: adapt-algorithm
#+begin_src R  :results output graphics :exports results  :file fig/adapt-algorithm.pdf
orig_data = read.table("./logs/adaptive_algorithm/total_times_orig.csv")
opt_data = read.table("./logs/adaptive_algorithm/total_times_adaptive.csv")

# Speedups of Constant Mode
opt_data[, "1"]  <- orig_data[, "c2"]  / orig_data[, "c2"]
opt_data[, "2"]  <- orig_data[, "c2"]  / opt_data[, "c2"]
opt_data[, "4"]  <- orig_data[, "c4"]  / opt_data[, "c4"]
opt_data[, "8"]  <- orig_data[, "c8"]  / opt_data[, "c8"]
opt_data[, "16"] <- orig_data[, "c16"] / opt_data[, "c16"]
opt_data[, "24"] <- orig_data[, "c24"] / opt_data[, "c24"]
keep <- c("nodes", colnames(opt_data)[grep("^[1-9]", colnames(opt_data))])
speedup_constant <- opt_data[keep]

# Speedups of Precise Mode
opt_data[, "1"]  <- orig_data[, "p2"]  / orig_data[, "p2"]
opt_data[, "2"]  <- orig_data[, "p2"]  / opt_data[, "p2"]
opt_data[, "4"]  <- orig_data[, "p4"]  / opt_data[, "p4"]
opt_data[, "8"]  <- orig_data[, "p8"]  / opt_data[, "p8"]
opt_data[, "16"] <- orig_data[, "p16"] / opt_data[, "p16"]
opt_data[, "24"] <- orig_data[, "p24"] / opt_data[, "p24"]
keep <- c("nodes", colnames(opt_data)[grep("^[1-9]", colnames(opt_data))])
speedup_precise <- opt_data[keep]

df <- melt(speedup_constant ,  id = 'nodes', variable_name = 'threads')
df2 <- melt(speedup_precise ,  id = 'nodes', variable_name = 'threads')
g1<-ggplot(df, aes(x=nodes,y=value, group=threads, colour=threads)) + geom_line() + 
    scale_fill_hue() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),
    panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(), axis.title.x = element_blank(),
    legend.position="none") +  scale_y_continuous(breaks=c(1,2,3,4)) + ylab("Constant mode")

g2<-ggplot(df2, aes(x=nodes,y=value, group=threads, colour=threads)) + geom_line() + scale_fill_hue() + theme(axis.text.x = element_text(angle = -45, hjust = 0),
    panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(),
    axis.title.x = element_blank(), legend.position="right") +
    scale_x_continuous(breaks=c(1000,5000,10000,25000,50000,75000)) + 
    scale_y_continuous(breaks=c(1,2,3,4)) + ylab("Precise mode")

legend <- g_legend(g2)
lwidth <- sum(legend$width)

grid.arrange(arrangeGrob(g1, g2 + theme(legend.position="none"),
             main ="",sub="Amount of nodes simulated",
             left = "Speedups"),legend, 
             widths=unit.c(unit(1, "npc") - lwidth, lwidth), nrow=1)
#+end_src


* Emacs Setup                                                      :noexport:
  This document has local variables in its postembule, which should
  allow org-mode to work seamlessly without any setup. If you're
  uncomfortable using such variables, you can safely ignore them at
  startup. Exporting may require that you copy them in your .emacs.

# Local Variables:
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (sh . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes 
#                       '("sigalt" "\\documentclass{sig-alternate}"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")))
# eval:    (add-hook 'org-babel-after-execute-hook 'org-display-inline-images) 
# eval:    (add-hook 'org-mode-hook 'org-display-inline-images)
# eval:    (add-hook 'org-mode-hook 'org-babel-result-hide-all)
# eval:   (setq org-babel-default-header-args:R '((:session . "org-R")))
# eval:   (setq org-export-babel-evaluate nil)
# eval:   (setq org-latex-to-pdf-process '("pdflatex -interaction nonstopmode -output-directory %o %f ; bibtex `basename %f | sed 's/\.tex//'` ; pdflatex -interaction nonstopmode -output-directory  %o %f ; pdflatex -interaction nonstopmode -output-directory %o %f"))
# eval:   (setq ispell-local-dictionary "american")
# eval:   (eval (flyspell-mode t))
# End:
