#+STARTUP: logdone
* May
** 13 [100%]
   - [X] Read Simix and section 3 of the paper
** 14 [100%]
   - [X] Revisit Orgmode emacs (no more vim orgmode)
   - [X] Revisit Tutorials, deploy,platform, play w/code
   - [X] Read Simix code, try to understand the main parts of it.
** 15 [100%]
   - [X] understand how to run experiment.
** 16 [100%]
   - [X] Figure out how to get the times of the previous exp. (system
     time, in log folder)
   - [X] deploy a parapluie node in rennes. Somehow it refuses to give
     me a node there, so I got a parapide node. Its not the same, but
     at least I am able to run the simulation.
   - [X] Install newer version of simgrid. I struggled a lot here,
     since it seems there are some files missing in
     examples/msg/chord/ and examples/platform/
   - [X] Figure out how to run with different ammount of threads --cfg
     flag for simgrid simulations

     
** 19 [100%]
   - [X] Figure out how to run all the experiments using a script
   - [X] Schedule a Launch with all the experiments. Run in
     parapluie-9.rennes.grid5000.fr

** 20 [100%] 
   - [X] Run simulation in G5K, using the last version available of
     [[https://gforge.inria.fr/projects/simgrid/][simgrid]] Anyway, the simulation was too slow, couldnt finish
     it. Had some unexpected exceptions too.

** 21 [100%]
   - [X] Run te experiment available at revision 918d6192, but with
     the newer revision. NOTE: this didnt work, the script to run the
     experiment is simpler than the one I actually need
   - [X] Run modified script on the older version and see what happens
   - [X] Run the scripts of 2011_parallel on the older version and see
     what happens
   - [X] Run the scripts of 2011_parallel on the newer version and see
     what happens. Still slower than the paper, at least in constant
     sequential mode.
** 22 [100%] Simulation is slow, even with the same parameters (like the older version). 
   - [X] Try with other runtime options, lile --cfg=config
   - [X] Try to figure out what the bottleneck is
   - [X] Learn to use FlameGraph
** 23 [100%]
   - [X] read and write down simix things
   - [X] try to use perf to understand performance ussues

     
** 26 [100%]
   - [X] Read and write down things about thread synchronization.
** 27 [100%]
   - [X] profilers: poor man profiler, perf, gprof.
   - [X] learn about thread synchronization.

** 28 [100%]
   - [X] Profiling to find bottleneck in simulation (poorman's, perf,
     gprof, flamegraph)
    
* June
** 2 - 6 SUD'14

** 10 [100%]
   - [X] Make script to test performance on different versions.
   - [X] Start tests performance in different versions of SG, using a
     "bisection" methodology.

To run (old) performance regression test: ./SGXP.pl --site=nancy
--cluster=graphene --test=chord,goal
--rev="e32a2a561ef184dc9ef8cdaf25759bba6b2ea198,71c9241aa27344f9f8f02f3c1272af4556691713,f95108e7e5bcb66bba76a24c1c433eee710e38e0"
** 11 [100%]
   - [X] Start with optimization of dynamic threshold of paper, have
     an initial version working (not necessarily more efficient).
** 12 [100%]
   - [X] Performances timings of dynamic threshold algorithm.
** 13 [100%]
   - [X] Run performance regression test.
   - [X] Finish performance timings of dynamic threshold algorithm,
     check if it is really working. Plot a graph.
   - [X] Try to start with the cores optimization of parmap.


** 16 [100%]
   - [X] Re run Performance regression test with fixed amount of
     commits. For version 3.9 and ahead there are ~2221 commits (until
     3.11 release, +2270 until HEAD)
** 17 [100%]
   - [X] Debug dynamic threshold algorithm
** 18 [100%]
*** DONE make a Rstudio file [100%]
  CLOSED: [2014-06-18 Wed 13:57]
**** DONE Transcript paper schema into file
     CLOSED: [2014-06-18 Wed 17:02]
**** DONE graph (Gnuplot to R)
     CLOSED: [2014-06-18 Wed 17:02]
** 19 [100%]
*** DONE start with pinpoint test
    CLOSED: [2014-06-20 Fri 09:30]
** 20 [100%]
*** DONE Fast performance tests to pinpoint the faulty commit & fix it. [66%]
    CLOSED: [2014-06-24 Tue 09:25]
**** DONE Adapt script to do it
   CLOSED: [2014-06-18 Wed 17:01]
**** DONE Run it in a dicotomy way: first the last commit, then some older commit, then a commit in the middle... until find the faulty one.
   CLOSED: [2014-06-20 Fri 17:21]
     Faulty commits-> well, actually I think there are dozens at least, since the performance drops down with the inclusion of "surf++"


** 23 [100%]
*** DONE start with pinpointing of surf branch
    CLOSED: [2014-06-24 Tue 09:26]
** 24 [100%]
**** DONE Explore logs of execution of C/C++ versions. Explore surf layer, searching for bug, using Paul suggestions.
     CLOSED: [2014-06-25 Wed 09:55]
** 25 [100%]
   - [X] Keep digging in branches. Special atention to hypervisor one
** 27 [100%]
*** DONE Apparently, the perforance drop was introduced in smx_network:1195 (compare with an older commit). make 2 tests: one with the code similar to the older version, one with the if blocks "right". 
    CLOSED: [2014-06-27 Fri 17:29]
    

** 30 [100%]
  - [X] Prepare scripts and code for experiments.

* July
** 1 [100%]
   - [X] Prepare code to run Amdahl Benchmark. Dont forget to set
     log_critical to the final print of Amdahl law. Then run exp. the
     same way without amdahl benchmark. In the table will be the final
     times, in the logs will be the times of the parallel/sequential
     execution too.
    CLOSED: [2014-07-01 Tue 16:00]
** 2 [100%]
*** DONE Run portion of experiments (<10k)
    CLOSED: [2014-07-02 Wed 17:56]
**** DONE without amdahl benchmark
     CLOSED: [2014-07-01 Tue 16:03]
**** DONE with amdahl benchmark
     CLOSED: [2014-07-02 Wed 17:55]
** 3 [100%]
   - [X] benchmark experiment for SR. Still not working.
** 4 [100%]
*** DONE fix exp. of benchmark per SR.
    CLOSED: [2014-07-04 Fri 16:47]


** 7 [100%]
   - [X] plot, Rstudio, organize data
** 8 [100%]
*** DONE work on Amdahl section on Rstudio.
    CLOSED: [2014-07-08 Tue 15:30]
**** DONE from the logs, get sequential and parallel times (python)
    CLOSED: [2014-07-08 Tue 15:28]
**** DONE From the normal logs, get completely sequential times (user+system)(python)
     CLOSED: [2014-07-08 Tue 16:21]
** 9 [100%]
*** DONE work on Rstudio
    CLOSED: [2014-07-10 Thu 10:57]
** 10 [100%]
*** DONE get better data and plot 2nd graph of intro.
    CLOSED: [2014-07-11 Fri 11:22]
** 11 [100%]
*** DONE run sr experiment with more nodes and gather data. The plot is still weird.
    CLOSED: [2014-07-15 Tue 09:50]


** 15 [100%]
*** DONE keep working on SR round times plot
    CLOSED: [2014-07-17 Thu 09:41]
**** DONE make more experiments from the laptop
     CLOSED: [2014-07-17 Thu 09:41]
** 16 [100%]
   - [X] valgrind and gdb to track the bug in chord. Found a
     corruption in memory, still dont find the bug
** 17-18 [100%]
   - [X] Analyze stacks, the bug has its origins on the SR benchmarks


** 21 [100%]
   - [X] Using gdb. Found where the SIGSEV comes from: raw_swapcontext
     from suspend_serial of smx_context_raw.c. The problem is with the
     address of the next_context The problem seems to be with the way
     'i' index was computed in smx_ctx_raw_suspend_serial. I fixed it
     and now it works.
** 22 [100%]
   - [X] Found 2 more segfaults: when nthreads=1 using the SR
     benchmarks, and when the sizes are too big (with or without
     benchmarks) in chord simulation.
   - [X] Re run experiments with SR with smaller sizes, in the cluster
** 23 [100%] 
   - [X] found that the 'performance fix' might be what causes the
     segfaults in some scenarios
** 24 [100%]
   - [X] Fixed potential SIGSEV when removing communication action
     when a SURF communication action is finishes (SIMIX_post_comm)
** 25 [100%]
   - [X] Make new experiments with bigger sizes: speedup (amdahl,
     /log_amdahl), SR (/log_sr), and raw (/log)(to compare
     intrusiveness of experiment). Use parapluie
   - [X] Prepare "micro" amdahl timings. That would be I)c). Use SR
     experiment, one with 1 thread (seq), one with better parallel
     version. Average timings of each amount of processes and then
     calculate speedup of each amount of processes


** 28 [100%]
   - [X] run SR like this: adding all the times of the parallel
     execution; taking the maxtime of the parallel execution. That way
     I get the sequential time of each SR, and the parallel time.
   - [X] generate 300000, 1000000 nodes files again, there are some
     problems with the current ones. Run final part of experiments
     with it(raw, amdahl, sr)
   - [X] Migrate from Rstudio to emacs
** 29 [100%]
   - [X] Work on better plots in rstudio. Work on report
** 30 [100%]
   - [X] fix SR
   - [X] keep working with report
** 31 [100%]
   - [X] organize data of experiments. Check that I have everything
     and probably relaunch some exp. to update data with the fixed
     version of SimGrid

* August
** 1 [100%]
   - [X] work on benchmark not intrusive
** 4 [100%]
   - [X] make new speedup plot with the bigger sizes (already got the
     logs). with 2,4,8,16,24 threads
   - [X] put numbers below bars in sr-distribution
** 5 [100%]
   - [X] get new data and make a decent sr-times plot,
   - [X] make sr-par-threshold with dots and add smooth line.
** 6 [100%]
   - [X] Start with optimizations of parmap.
** 7 [100%]
   - [X] Finish with binding cores details.
   - [X] Do some benchmarks with that.
   - [X] Write section in report, plot a graph
** 8 [100%]
   - [X] Check cache misses of previous/optimized version with perf.
   - [X] Start with parmap between N cores.
** 11 [100%]
   - [X] find why I get a deadlock running chord -> commit
     b533e2f7a6f6ebf750a96243804688169d2e6d9e, change only the
     #define. Also, check that the code at SIMIX_post_comm is deleted.
** 12 [100%]
   - [X] Still have some random deadlocks at the end of the
     simulation.Random. have to do some pinpoint, beginning from the
     performance fix.
** 13 [100%]
   - [X] Fix deadlock: delete SIMIX_process_cleanup(smx_proc) from
     src/msg/msg_process.c, why is that there?
   - [X] Do timings with maestro sleep+(N-1)
 
** 14 [100%]
   - [X] Timings maestrosleeps + (N-1) with 100k, 300k
** 18 [100%]
   - [X] Put code of bindings in order.
   - [X] Exp. intrusiveness with 3k, 5k, 10k, 25k, 50k, 75k nodes. (in
     progress)
** 19 [100%]
   - [X] Work with ideas of adaptative algorithm. Do some meditions if
     have time.
   - [X] Tons of meditions and benchmarks of parmap opt.,
     intrusiveness, amdahl speedup.

** 20 [100%]
   - [X] Amdahl speedup with 300k, 500k
   - [X] Short tests with adaptive algorithm in cluster.
   - [X] graph to show times of parmapN, parmapN-1 and original (al)
   - [X] Polish some things in report, like the plots with newer data,
     and some numbers using that data.
** 21 [100%]
   - [X] Write down things about binding threads.
   - [X] Run a new benchmark with the adaptive algorithm
** 22 [100%]
   - [X] Significant improve on adaptive algorithm plot. Write
     conclusions of this.
   - [X] Run remaining Amdahl experiments.
** 25 [100%]
   - [X] Finally, got a nicer dataset/plot of sr-par-threshold.
   - [X] Start Busy Waiters exp.
** 26 [100%]
   - [X] Constant vs. Precise with sr-par-threshold plot.(use a node
     or something, this TAKES time)
   - [X] Keep w/busy waiters optimization.
   - [X] Amdahl: Do plot with data gathered
** 27 [100%]
   - [X] Make more Constant mode experiments with sr-speedup to get
     better shape in plot
   - [X] plot busy_waiters vs. futex,check if there is some
     improvement.
   - [X] Use node to process the data of sr-counts
** 28 [100%]
   - [X] Process tables of sr-times plot and save them to generate
     latex faster.
   - [X] Work on report (organize plots, sections)

** 29 [100%]
   - [X] General cleanup of repository: erase old folders, update
     scripts, organize everything.
   - [X] Add documentation to scripts.

* September
** 1 [100%]
   - [X] comments about how to regenerate data in report.
   - [X] Re-run busy-waiters. Re-run speedup of scheduling rounds.

** 2 [100%]
   - [X] Integrate Luka script with mine. Make it more portable.
   - [X] Work on sr-par-threshold (plot again with new data, compare
     to old plot. Plot only with specific settings: certain amount of
     threads and sizes)
** 3 [100%]
   - [X] transforms files to csv.
   - [X] cleanup R code.
   - [X] modify logs paths in report. Add code to Data Analysis
     section.
** 4 [100%]
   - [X] Recheck plots, redo the sr-times, sr-par-thresh, busy
   - [X] new sr-distribution plot
** 5 [100%]
   - [X] Fix bugs of sr-distribution R code

** 8 [100%]
   - [X] add units/labels to plots. Add conclusions to plots.
   - [X] Take uniform distribution of sizes (not choose them
     randomly. Not choose 1000, 5000, 10000. Choose them equally
     distributed).
   - [X] Put host info in each chord_100*
   - [X] Change scripts to generate .csv, not tables.
** 9 [100%]
   - [X] test remote emacs, examples 
   - [X] test new benchmark with SR experiment
** 10 [100%]
   - [X] R remote execution + emacs, working fine
   - [X] G5K: create an image to accelerate a node setup. Check proxy
     configuration and set keys to clone SimGrid+report.
** 11 [100%]
  - [X] Work on portability&reproducibility of report. Rename/cleanup
    logs folders in a proper way.
  - [X] get_times.py now more general (can gather amdahl times,
    usr+sys, elapsed)
** 12 [100%]
  - [X] Make image of kadeploy better (add .vimrc, proxy settings,
    make setup.sh file and add it)
  - [X] Write about busy waiters comparison
  - [X] aesthetics of report.org

** 15 [100%]
  - [X] Make plots in same column of the same size (R)
** 16 [100%]
CUANDO SE AUMENTAN LOS THREADS AUMENTAN LAS SCHEDULING ROUNDS, POR???
  - [X] Integrate sr-distribution in the produced .pdf
  - [X] Drop everything related to the constant model. Precise mode
    needs to become more efficient.
  - [X] Ensure that every graph has a caption, a legend if it presents
    more than one curve, and that the axis are explained.
  - [X] Finish experiments of Amdahl with 1m nodes.(Precise mode)

** 17 [100%]
   - [X] Add setup scripts to report.
   - [X] launch experiments for speedup, with a more uniform
     distribution of sizes
** 18 [100%]
  - [X] Integrate testall script, scripts to generate
    platforms/deployment files to report
  - [X] Keep with standar benchmark in g5k

** 19 [100%]
  - [X] Delete get_sr_counts.py. The same thing can be achieved with oneline bash script.
  - [X] Add the used algorithm in section 5.2.
  - [X] Sizes, threads and log_folder should be defined in a separate
    block of code and then use them trhoughout the file in the same sh
    session

** 22 [100%]
  - [X] Improve algorithm in report. Describe variables, better names,
    erase timers
  - [X] Absolute numbers in section 3.1 (current
    speedup).Table+memoryconsumption
** 23 [100%]
  - [X] Test new changes in scripts in report.
  - [X] Add get_times script
  - [X] Integration test: run exp, gather data, do plot.

** 24 [100%]
  - [X] Put code snippets to change dir and redefine BASE_DIR and name them. They are useful.
  - [X] Improve mem consumpion table generation. 
** 25 [100%]
  - [X] Plot proportions of SR as stacked chart.
  - [X] Start launching the remaining exps.
** 26 [100%]
  - [X] Make new SR chart. Run pertinent experiments (This was a half-day)
** 29 [100%]
  - [X] Prepare code to launch remaining experiments. Modify adapt. algorithm to save threshold
  - [X] Better timings code for SR exp. Push that.
** 30 [100%]
  - [X] Fix bug in adaptive algorithm and launch exps.  Note: the
    evolution of the threshold over a simulation strongly dependends
    on the initial value of the threshold. I observed that there's a
    general tendence to increase/decrease the threshold in the same
    points of a simulation, but its values are not the same.
  - [X] Idea: launch timers every 100 SR may improve performance?. Answer: No.

* October
** 1 [100%]
  - [X] I ran almost all experiments with adaptive algorithm
  - Kademlia seems like an interesting example to benchmark.
** 2 [100%]
  - [X] Start benchmark of SR (In Progress)
  - [X] Start benchmark of busy waiter. (In Progress) Add conclusion with big sizes when finishing experiments.
  - [X] Start benchmark of binding (In Progress)
  - [X] Update plots of SR. Make report coherent with these plots.
  - [X] add memory consumption plots
** 3 [100%]
  - [X] draft of the conclussion
  - [X] Keep with benchmarks of SR, binding, busy, adaptive.
** 6 [100%]
  - [X] Update adaptive threshold in report.
  - Idea: instead of using the old method to calculate the optimal
threshold, do this: Use adaptive algorithm (or not), and do
experiments with differents tresholds. The one which gets more
speedup, the more optimal threshold it is.

** 7 [100%]
  - [X] In Progress: experiments of today (order): busy, binding, with all enabled (core binding, adaptive, real threshold)
  - [X] sr-times and sr-par-threshold with .png's. Fix captions
** 8 [100%]
  - Adaptive algorithm: instead of calculate the last ratio, better
    calculate an average value of ratios during the simulation. That
    would solve the problems of noise at the end, right?

  - Adaptive algorithm: proved to be better if we calculate ratio with
    the total amount of time, instead of the new calculated ratio.
    Anyway, keep the old logs, and create new ones, then compare them.

  - [X] Adaptive algorithm: keep average num of processes that run in
    parallel and sequential. That way, if a certain amount dont cross
    the average value, dont modify the threshold, but if it surpass
    the average (and is under/over the current treshold) do the
    benchmark and update the treshold This way we could avoid that
    extremely giant/small numbers introduce noise in the estimation.

** 9 [100%]
  - [X] Test new adaptive algorithm (In Progress).  Oh, found a
    possible drawback that makes simulations >500k nodes slow: using
    that f*ing sqrt() funtion.  I replaced that for an equivalent
    arithmetical expression, but still have to check if this fix the
    problem.
** 10 [100%]
  - [X] Fix adapt. problem with big sizes (slow). launch exps.
** 13 [100%]
  - [X] Change timer in SR: we are using CPU time, that gets
    multiplied for each CPU when we are using threads. Better use
    elapsed time and see what happens. Made new plot and modify
    report.
  - [X] Test new change in adaptive algorithm. It is working, apparently

** 14 [100%]
   - [X] Update report with new algorithm and results.

** 15 [100%]
   - [X] worked on definitive plots.
   - [X] adaptive algorithm now does not drop performance (so much) with big sizes.

** 16[100%]
   - [X] plot avg in sr-times
   - [X] Run experiment with threshold, plot the evolution of threshold.
** 17 [100%]
   - [X] keep testing evolution of threshold
** 20 [100%]
   - [X] Run experiment with all optimizations enabled (chord/kademlia. In progress)
   - [X] Fix heuristic in report.

** 21 [100%]
   - [X] finish all-enabled exp with chord/kademlia. Plot.

** 22 [100%]
   - [X] finish and check report

* Mixing Optimizations 
(not included in report)

The changes proposed are independent of each other, a fact
that naturally leads to think that maybe a greater speedup can be
obtained by mixing them.  A final benchmark with the optimal threshold
estimated, along with the adaptive threshold heuristic, physical
binding of threads and busy waiters is then compared to the original
times of the examples to get what we have achieved.

* Useful info:
** List of recommended packages/software to run experiments
   - Emacs24
   - org-mode8.2
   - ess
   - R
   - texlive
   - cmake make gcc git libboost-dev libgct++ libpcre3-dev linux-tools
     gdb liblua5.1-0-dev libdwarf-dev libunwind7-dev valgrind
     libsigc++
** To deploy a node with a personalized image
   1) oarsub -p 'cluster="parapluie"' -l nodes=1,walltime=8:00 -I -t deploy
   2) kadeploy3 -f $OAR_NODEFILE -e debian-simgrid -k
   3) connect to the deployed node and run ./post_deploy.sh
   4) ssh-keygen, put .pub in authorized_keys of rennes frontend
   5) R packages and libraries needed should be installed.

** To run classical chord experiment
   - Steps to connect to G5K and deploy SimGrid:
     1) connect to G5k, connect to Rennes, try to ask a parapluie node:
       oarsub -p "cluster='parapluie'" -l nodes=5,walltime=2 -I -t deploy
	Aks for an specific node, for example:
       oarsub -p 'cluster="parapide"' -l {'network_address in ("parapide-8.rennes.grid5000.fr")'}/nodes=1,walltime=5:00 -I -t deploy
     2) apt-get update && apt-get install cmake make gcc git libboost-dev libgct++ libpcre3-dev linux-tools gdb liblua5.1-0-dev libdwarf-dev libunwind7-dev valgrind libsigc++
     3) copy simgrid from Rennes frontend
     4) cmake -Denable_compile_optimizations=ON -Denable_supernovae=OFF -Denable_compile_warnings=OFF -Denable_debug=OFF -Denable_gtnets=OFF -Denable_jedule=OFF -Denable_latency_bound_tracking=OFF -Denable_lua=OFF -Denable_model-checking=OFF -Denable_smpi=OFF -Denable_tracing=OFF -Denable_documentation=OFF .
     5) make && make install && sudo chmod 777 /usr/local/lib/libsimgrid.so.<version_number>
     6) example run:
        ./chord One_cluster_nobb_1000_hosts.xml chord1000.xml --log=root.thres:critical --cfg=contexts/stack_size:16 --cfg=contexts/guard_size:0 --cfg=network/model:Constant --cfg=network/latency_factor:0.1
	./chord One_cluster_nobb_10000_hosts.xml chord10000.xml --cfg=contexts/stack_size:16 --log=root.thres:critical --cfg=network/model:Constant --cfg=network/latency_factor:0.1 --cfg=contexts/nthreads:4
	./chord One_cluster_nobb_10000_hosts.xml chord10000.xml --cfg=contexts/stack_size:16 --log=root.thres:critical --cfg=maxmin/precision:0.00001 --cfg=contexts/nthreads:4 --cfg=maxmin/precision:0.00001
     7) to run test, this should work:  ./testall.sh path/to/simgrid/ 3.11
     8) Dont forget to modify script to copy logs to home folder (otherwise they will lay on the node, and they will be erased)
     9) To copy something from rennes to the deployed node: scp -r rtortilopez@rennes.grid5000.fr:path/to/file . 
   - To print current revision hash git rev-parse --short HEAD
     1) git log --pretty=oneline --pretty=format:"%h %s"  --since 17/11/2013 --until 23/01/2014 |  less
     2) If it fails to allocate stack with big amount of nodes, then use --cfg=contexts/guard_size:0

for file in sr_*; do tail -n +0 $file >> sr_total_1000.log; done
** Profilers:
    1) perf record ./chord ...
    2) perf record -g -e cpu-clock ./chord and then:
    3) perf script | stackcollapse-perf.pl | flamegraph.pl > myapp.svg
    4) script gdb poorman profiler of Gabriel | stackcollapse-gdb.pl | flamegraph.pl > myapp.svg . Example:
       Run chord:
       ./chord One_cluster_nobb_10000_hosts.xml chord10000.xml --log=root.thres:critical --cfg=contexts/stack_size:16 --cfg=network/model:Constant --cfg=network/latency_factor:0.1 &
       and: 
       ./poorman.sh 1000 0.1 $(pidof chord) | ./stackcollapse-gdb.pl | ./flamegraph.pl > myapp.svg
    5) To copy from node to laptop: scp root@<node_number>.rennes.grid:~/SimGrid/examples/msg/chord/myapp.svg .
    6) poor man's profiler
    7) try with compilation flag -fno-omit-frame-pointer
** To run SG-PRT (Performance regression test):
repository: git+ssh://rtortilo@scm.gforge.inria.fr//gitroot/simgrid/simgrid-perf-reg-tests.git
1) set up a node somewhere.
2) apt-get update && apt-get install cmake make gcc git libboost-dev libgct++ libpcre3-dev linux-tools gdb r-base ruby gem . If possible, gem install rserve-client
3) To install libraries of R, use script download_r_packages.sh
4) Start Rserve daemon in R (library(Rserve); Rserve())
5) set number of commits wanted to analyze in SG-PRT.rb
6) ./SG-PRT.rb

** Pinpoint tests
1) To get a list of specific commits (and count them,etc):
   git log 772d63c..ce1289d --pretty=oneline | head -n570 | less
2) If compilaton of chord fails for version 3.10 (or whatever else),
   try copying the content of folder src/ to usr/local/src
** Useful commands
- To rename bunch of files (useful for organizing logs):
  rename 's/^/2_/' sr_5000_threads*
  (basically rename <pattern_prefix>  <pattern_group_files>)
- To get the sizes simulated so far in a folder of logs:
  ls | tr "_" " " | awk '{print $2}' | uniq | head -23 | sort -g
* Paper
** Problem Analysis
  1) Optimal threshold for Amdhal. Which portion of the problem is
     paralellizable. Which not. Why is hard?(a) Two graphics: #SD
     vs. #process (b) && #SD vs. time(SD) (c)
     
** Find threshold between sequential/parallel
  1) test over real runs. not simulations
  2) geometric mean
Once I get the basis threshold:
  3) adaptative algorithm to choose how many threads wake up in each round:
     This is what is already done (smx_ctx_*_runall):
        * if #proc<threshold  --> sequential
        * elif #proc>threshold  --> parallel
     Change threshold dinamically:
     There are timers already defined in the code, but they are not used to do this:
     * if #proc<thresh -->  sequential exec. (chrono) and then seq_time += chrono; seq_amount += #proc;
     * else ------------->  parallel exec. (crhono) and then par_time+=chrono; par_amount+=#proc;
     After 5 sched round:
                (a)                     (b)
     * if seq_time/seq_amount > par_time/par_amount THEN --thresh (c - coeff x a/b)
     * else ++thres
  4) parallel threshold of parmap in cache?
  5) Fast Init: save threshold on disk?

Versions of SimGrid:
SimGrid3.11.1 e32a2a561ef184dc9ef8cdaf25759bba6b2ea198
SimGrid3.8    71c9241aa27344f9f8f02f3c1272af4556691713
SimGrid3.9    f95108e7e5bcb66bba76a24c1c433eee710e38e0
SimGrid3.10   772d63cca583f5d16096fa9f487b4ab07d9af8f0

** Optimizations
  1) bind each tread to one core [X]
  2) implement parmap between N cores: maestroawake+(N-1)(default); [X]
                                       The key for the next 2 optimizations relies on finding how to sleep master,
                                       the rest involves only creating N threads or N-1 threads:
                                           maestrosleep+(N)
                                           maestro sleep+(N-1)
  3) busy waiters with different #proc. (1-5 --> sequential; 6-20 --> 4busy waiters; 20-inf --> 16 busy waiter).[X]
  Performance regression testing
  
** Hot topics
1) we are near amdhal paralelism threshold, because there are no more
   things to paralellize.
2) we want to run in parallel not only the threads but also the events
   in each thread?
3) we want to find the independent events and run them in parallel.



