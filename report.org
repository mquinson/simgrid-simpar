
#+TITLE: Parallel and Distributed Simulation of Large-Scale Distributed Applications
#+AUTHOR:  Ezequiel Torti Lopez, Martin Quinson
#+OPTIONS: H:5 title:nil date:nil author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil
#+STARTUP: indent hideblocks
#+TAGS: noexport(n)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+PROPERTY: session *R* 

#+LATEX_class: sigalt
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{ifthen,figlatex}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage[american]{babel}
#+LATEX_HEADER: \usepackage{url}\urlstyle{sf}
#+LATEX_HEADER: \usepackage{amscd}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage[noend]{algpseudocode}
#+LATEX_HEADER: \renewcommand{\algorithmiccomment}[1]{// #1}


* Motivation and Problem Statement

Simulation is the third pillar of science, allowing to study complicated
phenomenons through complex models. When the size or complexity of the studied
models becomes too large, it is classical to leverage more resources through
Parallel Discrete-Event Simulation (PDES).

Still, the parallel simulation of very fine grained applications deployed on
large-scale distributed systems (LSDS) remains challenging. As a matter of fact,
most simulators of Peer-to-Peer systems are sequential, despite the vast
literature on PDES over the last three decades.

dPeerSim is one of the very few existing PDES for P2P systems, but it presents
deceiving performance: it can achieve a decent speedup when increasing the
amount of logical processes (LP): from 4h with 2 LPs down to 1h with 16 LPs.
But it remains vastly inefficient when compared to sequential version of
PeerSim, that performs the same experiment in 50 seconds only. This calls for a
new parallel schema specifically tailored to this category of Discrete Event
Simulators.

Discrete Event Simulation of Distributed Applications classically alternates
between simulation phases where the models compute the next event date, and
phases where the application workload is executed.  We proposed

in~\cite{previous} to not split the simulation model across several computing
nodes, but instead to keep the model sequential and execute the application
workload in parallel when possible. We hypothesized that this would help
reducing the synchronization costs. We evaluate our contribution with very fine
grained workloads such as P2P protocols. These workloads are the most difficult
to execute efficiently in parallel because execution times are very short,
making it very difficult to amortize the synchronization times.

We implemented this parallel schema within the SimGrid framework, and showed
that the extra complexity does not endangers the performance since the
sequential version of SimGrid still outperforms several competing solutions when
our addition are present but disabled at run time.

To the best of our knowledge, it is the first time that a parallel simulation of
P2P system proves to be faster that the best known sequential execution. Yet,
the parallel simulation only outperforms sequential one when the amount of
processes becomes large enough. This is because of the pigonhole principle: when
the amount of processes increases, the average amount of processes that are
ready to run at each simulated timestamp (and can thus run in parallel)
increases. When simulating the Chord protocol, it takes 500,000 processes or
more to amortizing the synchronization costs, while the classical studies of the
literature usually involve less processes.

The current work aims at further improving the performance of our PDES, using
several P2P protocols as a workload. We investigate the possible inefficiency
and propose generic solutions that could be included in other similar simulators
of large-scale distributed systems, be them P2P simulators of cloud, HPC or
sensornets ones.

This paper is organized as follows: Section \ref{sec:context} recaps the SimGrid
architecture and quickly presents the parallel execution schema detailed in
\cite{previous}. Section \ref{sec:problem} analysis the theoretical performance
bound, and discusses the previous work at the light of the Amhdal law. Section
\ref{sec:parallel} explores several trade-offs for the efficiency of the
parallel sections. Section \ref{sec:adaptive} proposes an algorithm to
automatically tune the level of parallelism that is adapted to the simulated
application. Section \ref{sec:cc} concludes this paper and discusses some future
work.

* Context
#+LaTeX: \label{sec:context}

In the previous work ~\cite{previous} we proposed to parallelize the
execution of the user code while keeping the simulation engine
sequential.  This is enabled by applying classical concepts of OS
design to this new context: every interaction between the user
processes (from now on, user processes and processes mean the same
thing) and the simulated environment passes through a specific layer
that act as an OS kernel.

A novel way to virtualize user processes (\emph{raw contexts}) was
crafted to improve efficiency and avoid unnecesary system calls, but
other ways to do this can be found for the sake of portability, such
as full featured threads, or POSIX ucontexts. A new data structure to
store the shared state of the system and synchronize the process
execution was implemented as well (\emph{parmap}).

A new specific layer that acts as the OS kernel was implemented in
SimGrid to emulate systems calls, called \emph{requests}, and each
time a user process want to interact with other process, or the kernel
itself, it raises a \emph{request}.  After that, the engine takes
control of the program and answer the \emph{requests} of each
process. This way the user processes can be parallelized in a safe
manner.

Experimental results showed that the new design does not hinder the
tool scalability, and even the sequential version is more scalable
than state of the art simulators.  The difficulty to get a parallel
version of a P2P simulator faster than its sequential counterpart was
also revealed in ~\cite{previous}, being the first time that a
parallel simulation of Chord runs faster than the best known
sequential implementation.

An interesting result showed in the previous work is that the speedups
only increased up to a certain point when increasing the amount of
working threads.  We also have proved that for small instances,
parallelism actually hinders the performance, and that the relative
gain of parallelism seems even strictly increasing with the system
size.

Now we are closer to the optimal Amdahl's law threshold, that means
that we have reach a limit on the parallelizable portions of the code
in our proposed model.  The remaining optimizations seek for a final
speedup, trying to get a better parallel threshold dynamically
depending on the simulation, and better performance of the threads
taking in count their distribution on the CPU cores and the different
synchronization modes (futex, POSIX primitives or busy waiters).


* Performance Analysis
#+LaTeX: \label{sec:problem}
** Current speedup achieved
# Also, the benchmarking not intrusive is here.
To get a baseline timings and a speedup plot starting from the current
version of SimGrid (3.11), benchmarks to measure the execution time of
a typical Chord simulation in Precise mode with different amount of
threads (1, 2, 4, 8, 16 and 24) were done.

The total times of a normal execution for the Chord simulation in the
precise mode are presented in the table \ref{tab:one}.

#+caption: Execution times of a normal execution of Chord with different sizes, serial and with 2 and 8 threads. The average memory consumption is reported in GB.
#+name: tab:one
|---+-------+---------+-------+---------+-------+---------+-------|
|   | nodes |  serial |   Mem |  2 thr. |  Mem. |  8 thr. |  Mem. |
| / | <>    |       < |     > |       < |     > |       < |     > |
|---+-------+---------+-------+---------+-------+---------+-------|
| # | 1k    | 0:00:04 |  0.03 | 0:00:07 |  0.03 | 0:00:09 |  0.03 |
| # | 5k    | 0:00:28 |  0.13 | 0:00:40 |  0.13 | 0:00:49 |  0.13 |
| # | 10k   | 0:01:03 |  0.25 | 0:01:20 |  0.26 | 0:01:35 |  0.25 |
| # | 50k   | 0:06:20 |  1.24 | 0:07:39 |  1.27 | 0:08:03 |  1.25 |
| # | 100k  | 0:13:34 |  2.47 | 0:15:36 |  2.53 | 0:15:50 |  2.50 |
| # | 300k  | 0:50:58 |  7.38 | 0:55:18 |  7.54 | 0:57:55 |  7.47 |
| # | 500k  | 1:38:16 | 12.30 | 1:34:15 | 12.47 | 1:35:10 | 12.45 |
| # | 1m    | 4:05:41 | 24.53 | 4:00:42 | 24.89 | 3:47:28 | 24.91 |
|---+-------+---------+-------+---------+-------+---------+-------|

As it can be seen in Figure \ref{fig:one.one}, the memory consumption
linearly increases with respect to the number of simulated nodes, and
shows that each node is using around 25 KB and 30 KB. A simulation
with 1000 nodes, has a peak memory consumption around 30 MB
(regardless of the amount of threads launched) and finishes in 4
seconds in a serial execution, and one with 1000000 nodes takes
24-25GB of memory and 3h47m to finish in the best case (parallel
execution with 8 threads).

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:one.one
#+caption: Memory consumptions reported in GB
#+results: memory-consumption
[[file:fig/memory-consumption.pdf]]

The actual speedup obtained can be seen in the Figure \ref{fig:one}.
It is clear from that graph that the real speedup with our parallel
model is obtained when the size of the problem is bigger than 300000
nodes.  This is a proof that what was proved in our previous work
~\cite{previous} is still valid.

Figure \ref{fig:one} also shows that increasing the number of threads
may not be the best option to increase performance, since the best
speedups are achieved with 2,4 and 8 threads. Some of the
optimizations proposed in section \ref{sec:parallel} show improvements
over the original versions with 16 and 24 threads, but their total
times are still behind the ones of the same simulations with lesser
amount of threads.

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:one
#+caption: Baseline performance of SimGrid 3.11. Speedups achieved using multithreaded executions against the sequential ones.
#+results: baseline-perf
[[file:fig/baseline-perf.pdf]]

# We want to see now is how far are we from the ideal speedup that
# would be achieved according to the Amdahl law.  For that, a benchmark
# test is run to get the timings of the sequential and parallel parts of
# the executions, and the calculate that speedup using the Amdahl
# equation.

# But first we want to prove that our benchmarks are not intrusive, that
# is, our measures of parallel and sequential times do not really affect
# the overall performance of the system. For that, the experiments are
# run with and without benchmarking, using the Precise mode, and then a
# comparison of both is made to find if there is a significative breach
# in the timings of both experiments.

# Using the Chord simulation, the experiment showed us that the maximum
# difference in the execution time of both versions is lesser than 10%
# in most of the cases, and is even lower with sizes bigger than 100000
# nodes, which allow us to conclude the benchmarking is, indeed, not
# intrusive.

** Parallelizable portions of the problem
This experiment is based on a typical Chord simulation, and the data
wanted is the following: ID of each Scheduling Round, time taken by
each Scheduling Round and number of process executed in each
scheduling round.

What we want to prove is that the limit on the speed up reached is due
to the fact that we are very closer to the line that define what is
parallelizable in our model and what is executed sequentially.  As it
can be seen in the Figure \ref{fig:two} , the amount of processes
computed by each scheduling round is only one most of the times
(between 50-60\%), so the parallel execution is not possible in that
instances. The remaining processes are executed in parallel due to the
parallel execution threshold already setted up in SimGrid (which can
be modified).

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:two
#+caption: Proportions of SR's having different numbers of processes to compute; according to the size of nodes simulated.
[[file:fig/sr-distribution.pdf]]

However, launching a small amount of processes is inefficient due to
the costs of synchronization of threads.  Even when Figure
\ref{fig:three} shows that the bigger the amount of processes in a SR,
the bigger the execution time, there is no speedup obtained from
executing small amounts of processes in parallel, as we will see in
Section \ref{sec:adaptive}. Also, in that section we will try to find
what is the optimal threshold between serial and parallel executions.

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:three
#+caption: Times of SR's sequential executions depending on the amount of processes of each SR.
#+results: sr-times
[[file:fig/sr-times.pdf]]

* Optimizations
#+LaTeX: \label{sec:parallel}
** Binding threads to physical cores
Regarding the multicore architectures (like almost every modern CPU),
parallelization through threads is well proved to be a good
optimization, as we said in Section \ref{sec:problem}. But there are
still some improvements that can be done.

Thread execution depends heavily on the operative system scheduler:
when one thread is \emph{idle}, the scheduler may decide to switch it
for another thread ready to work, so it can maximize the occupancy of
the cpu cores, and probably, run a program in a faster way. Or it may
just want to switch threads because their execution time quote is
over. When the first thread is ready to work again, the cpu core where
it was before might be occupied, forcing the system to run the thread
in another core. Of course this depend on which scheduler
implementation we are using.

Regardless of the situation, migration of threads between cores
entails an increase of CPU migrations, which in a big size simulation
can be detrimental for the performance.

In order to avoid these CPU migrations produced by a constant context
switching of threads, Glib offers a way to bind each thread to a
physical core of the CPU. Note that this is only available in Linux
platforms.

A Chord simulation was run in a parapluie node with 24 cores, binding
the threads to physical cores. The CPU migration was drastically
reduced (almost 97\% less migrations) in all the cases.  The relative
speedup with few threads (2, 4 and 8) was not big enough: x1.63 in the
best case, and x1.23 in average.  But when the simulation is run with
a bigger amount of threads (16 or 24), the impact of having less CPU
migrations is notable, being obtained speedups between x2.44 and
almost x15 (depending on the amount of threads and the size of the
simulation).  This proves that physical binding of threads to CPU
cores can be useful when a big amount of threads is needed.

** Parmap between N cores

Several optimizations regarding the distribution of work between
threads were proposed: the first option is the default one, where
maestro works with its threads and the processes are distributed
equitably between each thread; the second one is to send maestro to
sleep and let the worker threads do all the computing; the last one
involves the creation of one extra thread and make all this N threads
work while maestro sleeps.

The experiments were made using up to 75000 nodes in a Chord
simulation with Precise mode, but no performance gain was achieved. In
fact, the creation of one extra thread proved to be slightly more
slower than the original version of parmap, while sending maestro to
sleep and make its N-1 threads do the computation did not show any
improvement or loss in performance.

** Busy Waiting versus Futexes
SimGrid provides several types of synchronization between threads:
Fast Userspace Mutex (futex), the classical POSIX synchronization
primitives and busy waiters.  While each of them can be chosen when
running the simulation, futexes are the default option, since they
have the advantage to implement a fast synchronization mode within the
parmap abstraction, in user space only.  But even when they are more
efficient than classical mutexes, which run in kernel space, they may
present performance drawbacks.  In this section we compare the busy
waiters vs. futexes synchronization types, using the chord simulation
in both Precise mode, and we found that using busy waiters can result
in a speedup between x1.08 and x1.53 using few threads (up to 8)
against the same parallel version using futexes, and from 1.15 to 3
when the amount of threads is big enough (16 or 24).

TODO: conclusion about big sizes?

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:four
#+caption: Relative speedup of busy waiters vs. futexes in Chord simluation.
#+results:busy
[[file:fig/busy.pdf]]

** Performance Regression Testing

* Optimal threshold for parallel execution
#+LaTeX: \label{sec:adaptive}
** Getting a real threshold over simulations
The threshold wanted is how many processes are the right amount to be
executed in parallel when it is necessary, and when is it better to
execute them in a sequential way. Initially, what we want is to find
an optimal threshold for the beginning of any simulation.  For that
purpose, we have done a benchmark to get each SR execution time for both
parallel and serial executions, and calculated the speedup obtained in
each SR.

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:five
#+caption: Speedup of parallel vs. sequential executions of SR's, depending in the number of processes taken by each SR
#+results:sr-par-threshold
[[file:fig/sr-par-threshold.pdf]]

As it can be seen in Figure \ref{fig:five}, the speedup is bigger than one TODO

** Adaptive algorithm to calculate threshold
Finding an optimal threshold and keep it during all the simulation
might not always be the best option: some simulations can take more or
less time in the execution of user processes. If a simulation has very
efficient processes, or processes that don't work too much, then the
threshold could be inappropriate, leading to parallelize scheduling
rounds that would run more efficiently in a sequential way.  That's
why an algorithm for a dynamic threshold calculation is proposed.

The main idea behind this heuristic is to calculate the optimal number
of processes that can be run in parallel during the execution of the
simulation.

For that purpose, the times of five scheduling round are measured. A
performance ratio for both of the possible parallel and sequential
executions is calculated, simply by dividing the time taken by the
amount of processes computed.  If the sequential ratio turns to be
bigger than the parallel one, then the threshold is decreased, and
increased otherwise.


#+begin_latex
\begin{algorithm}
\caption{Adaptive Threshold}\label{adaptive-algorithm}
\begin{algorithmic}

\State
\Comment {Amount of parallel/sequential SRs that ran}
\State $parallel\_SRs, sequential\_SRs \gets \textit{1}$
\State
\Comment {Sum of times of par/seq SR's}
\State $seq\_time, par\_time \gets \textit{0}$
\State
\Comment {Number of processes computed in par/seq}
\State $process\_seq, process\_par \gets \textit{0}$
\State

\Procedure{RunSchedulingRound}{}

\If {computed five par/seq SR's}
\State $parallel\_SRs \gets \textit{1}$
\State $sequential\_SRs \gets \textit{1}$
\State $ratio\_seq \gets seq\_time/process\_seq$
\State $ratio\_par \gets par\_time/process\_par$
\State $sequential\_is\_slower \gets ratio\_seq>ratio\_par$
\If {$sequential\_is\_slower$}
\State decrease($parallel\_threshold$)
\Else
\State increase($parallel\_threshold$)
\EndIf
\State $seq\_time,par\_time \gets \textit{0}$
\State $process\_seq,process\_par \gets \textit{0}$
\EndIf

\State
\If {$processes\_to\_run >= parallel\_threshold$}
\State execute\_SR\_parallel()
\State $process\_par \gets get\_number\_of\_processes()$
\State $parallel\_SRs++$
\Else
\State execute\_SR\_serial()
\State $process\_seq \gets get\_number\_of\_processes()$
\State $sequential\_SRs++$
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}
#+end_latex

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:six
#+caption: Speedup achieved with Adaptive Algorithm. Chord simulation, Precise mode.
#+results: adapt-algorithm
[[file:fig/adapt-algorithm.pdf]]


This results in a relative improvement in performance. As it can be
seen on Figure \ref{fig:five}, the speedup is important in the 16 and
24 threads cases, reaching levels between 11 and 47 with small sizes
(1000, 5000, and 10000 nodes), while with fewer amounts of threads
(2,4,8) the speedup is not big, between 1.28 and 4.62.  This can be
explained by the fact that the algorithm choose more efficiently when
to launch a scheduling round in parallel, and while having a lot of
threads increases the costs of synchronization, an intelligent choice
of when to launch them will reduce that cost.

Regarding the memory consumption, the values remain the same in
general, as it can be seen in Table /ref{tab:two}

TODO: this table is not useful at all as it is (is just a test). We
need the logs of the experiments w/bigger sizes to show the real
speedup.

#+caption: Execution times (seconds) of the Adaptive algorithm, with 2,4 and 8 threads. The average memory consumption is reported in GB.
#+name: tab:two
|--------+--------+------+--------+------+--------+------|
|  nodes | 2 thr. |  Mem | 4 thr. |  Mem | 8 thr. |  Mem |
|      / |     <> |    < |      > |    < |      > |    < |
|   1000 |      7 | 0.03 |      8 | 0.03 |      9 | 0.03 |
|   5000 |     40 | 0.13 |     45 | 0.13 |     49 | 0.13 |
|  10000 |     80 | 0.26 |     87 | 0.26 |     95 | 0.25 |
|  50000 |    459 | 1.27 |    470 | 1.27 |    483 | 1.25 |
| 100000 |    936 | 2.53 |    947 | 2.54 |    950 | 2.50 |
|--------+--------+------+--------+------+--------+------|


* Conclusion
#+LaTeX: \label{sec:cc}


#+LaTeX: \onecolumn
#+LaTeX: \appendix
* Data Provenance
This section explains and show how to run the experiments and how the
data is saved and then processed.  Note: that all experiments are run
using the Chord simulation that can be found in examples/msg/chord
folder of your SimGrid install. Unless stated, all the experiments are
run using the futex synchronization method and raw contexts under a
Linux environment; in a node from 'parapluie' cluster of Grid5000.
The analysis of data can be done within this paper itself, executing
the corresponding R codes. Note that it is even possible to execute
them remotely if TRAMP is used to open this file (this is useful if
you want the data to be processed in one powerful machine, such as a
cluster).
** Modifiable Parameters
Some of the parameters to run the experiments can be modified, like
the amount of nodes to simulate and the amount of threads to use.
Note that the list of nodes to simulate have to be changed in both the
python session and the shell session.  This sessions are intended to
last during all your experiments/analysis.

This sizes/threads lists are needed to run the simulations, generate
platform/deployment files, and generate tables after the
experiments. Thus, is mandatory to run this snippets.

#+begin_src sh :session org-sh
BASE_DIR=$PWD
sizes=(1000 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 55000 60000 65000 70000 75000 80000 85000 90000 95000 100000 300000 500000 1000000)

threads=(1 2 4 8 16 24)
#+end_src

#+name: set_python_args
#+begin_src python :session
  SIZES = [1000]
  SIZES += [elem for elem in range(5000,100000,5000)]
  SIZES += [100000,300000,500000,1000000]
  THREADS = [1, 2, 4, 8, 16, 24]
  # All the benchmarks can be done using both modes, but note that this
  # paper uses only precise
  MODES = ['precise']
  nb_bits = 32
  end_date = 10000
#+end_src

** Setting up the machine
Install required packages to compile/run SimGrid experiments. If you
are in a cluster (such as Grid5000) you can run this file remotely in
a deployed node and still be able to setup your environment.  Run this
two code chunks one after other in order to create folders, install
packages and create required deployment/platform files.

If the [[setup_and_install]] snippet was run before, or everything is
already installed and set up, then check/modify the parameters of the
shell session with the snippets [[check_args]] and [[go_to_chord]]

#+name: setup_and_install
#+begin_src sh :session org-sh

# Save current directory where the report is
BASE_DIR=$PWD
apt-get update && apt-get install cmake make gcc git libboost-dev libgct++ libpcre3-dev linux-tools gdb liblua5.1-0-dev libdwarf-dev libunwind7-dev valgrind libsigc++
mkdir -p SimGrid deployment platforms logs fig
cd $BASE_DIR/SimGrid/
# Clone latest SimGrid version. You may have to configure proxy settings if you are in a G5K node in order to clone this git repository
git clone https://gforge.inria.fr/git/simgrid/simgrid.git .
SGPATH='/usr/local'
# Save the revision of SimGrid used for the experiment
SGHASH=$(git rev-parse --short HEAD)
cmake -Denable_compile_optimizations=ON -Denable_supernovae=OFF -Denable_compile_warnings=OFF -Denable_debug=OFF -Denable_gtnets=OFF -Denable_jedule=OFF -Denable_latency_bound_tracking=OFF -Denable_lua=OFF -Denable_model-checking=OFF -Denable_smpi=OFF -Denable_tracing=OFF -Denable_documentation=OFF .
make install
cd ../../
#+end_src

#+name: generate_platform_files
#+begin_src python :session :results output

# This function generates a specific platform file for the Chord example.
import random
def platform(nb_nodes, nb_bits, end_date):
  max_id = 2 ** nb_bits - 1
  all_ids = [42]
  res = ["<?xml version='1.0'?>\n"
  "<!DOCTYPE platform SYSTEM \"http://simgrid.gforge.inria.fr/simgrid.dtd\">\n"]
  res.append("<!-- nodes: %d, bits: %d, date: %d -->\n"%(nb_nodes, nb_bits, end_date))
  res.append("<platform version=\"3\">\n"
  "  <process host=\"c-0.me\" function=\"node\"><argument value=\"42\"/><argument value=\"%d\"/></process>\n" % end_date)
  for i in range(1, nb_nodes):
    ok = False
    while not ok:
      my_id = random.randint(0, max_id)
      ok = not my_id in all_ids
    known_id = all_ids[random.randint(0, len(all_ids) - 1)]
    start_date = i * 10
    res.append("  <process host=\"c-%d.me\" function=\"node\"><argument value=\"%d\" /><argument value=\"%d\" /><argument value=\"%d\" /><argument value=\"%d\" /></process>\n" % (i, my_id, known_id, start_date, end_date))
    all_ids.append(my_id)
  res.append("</platform>")
  res = "".join(res)
  f  = open(os.getcwd() + "/platforms/chord%d.xml"%nb_nodes, "w")
  f.write(res)
  f.close()
  return

# This function generates a specific deployment file for the Chord example.
# It assumes that the platform will be a cluster.
def deploy(nb_nodes):
  res = """<?xml version='1.0'?>
<!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid.dtd">
<platform version="3">
<AS  id="AS0"  routing="Full">
  <cluster id="my_cluster_1" prefix="c-" suffix=".me"
  		radical="0-%d"	power="1000000000"    bw="125000000"     lat="5E-5"/>
</AS>
</platform>"""%(nb_nodes-1)
  f = open(os.getcwd() + "/deployment/One_cluster_nobb_%d_hosts.xml"%nb_nodes, "w")
  f.write(res)
  f.close()
  return 

# Remember that SIZES was defined as a global variable in the first python code chunk in [[Modifiable Parameters]]
for size in SIZES:
  platform(size, nb_bits, end_date)
  deploy(size)
#+end_src

Optional snippets to check arguments and go to chord folder:

#+name: check_args
#+begin_src sh :session org-sh
echo $sizes
echo $threads
echo $BASE_DIR
#sizes=(1000)
#threads=(1 2)
#BASE_DIR=$PWD
echo $sizes
echo $threads
echo $BASE_DIR
#+end_src

#+name: go_to_chord
#+begin_src sh :session org-sh
cd $BASE_DIR/SimGrid/examples/msg/chord
echo $BASE_DIR
echo $sizes
echo $threads
make
#+end_src

** Scripts to run benchmarks
This are general scripts that can be used to run all the benchmarks
after the proper modifications were done.

#+name: testall
#+begin_src sh  :var SG_PATH='/usr/local' :var log_folder="logs" :session org-sh

# This script is to benchmark the Chord simulation that can be found
# in examples/msg/chord folder.
# The benchmark can be done with both Constant and Precise mode, using
# different sizes and number of threads (which can be modified).
# This script also generate a table with all the times gathered, that can ease
# the plotting, compatible with gnuplot/R.
# By now, this script copy all data (logs generated an final table) to a 
# personal frontend-node in Grid5000. This should be modified in the near
# future.

###############################################################################
# MODIFIABLE PARAMETERS: SGPATH, SGHASH, sizes, threads, log_folder, file_table
# host_info, timefmt, cp_cmd, dest.

# Path to installation folder needed to recompile chord
# If it is not set, assume that the path is '/usr/local'
if [ -z "$SG_PATH" ]
then
    SGPATH='/usr/local'
fi

# Save the revision of SimGrid used for the experiment
SGHASH=$(git rev-parse --short HEAD)

# List of sizes to test. Modify this to add different sizes.
if [ -z "$sizes" ]
then
    sizes=(1000 3000)
fi

# Number of threads to test. 
if [ -z "$threads"]
then
    threads=(1 2 4 8 16 24)
fi

# Path where to store logs, and filenames of times table, host info
if [ -z "$log_folder"]
then
    log_folder=$BASE_DIR"/logs"
else
    log_folder=$BASE_DIR"/logs/"$log_folder
fi

if [ ! -d "$log_folder" ]
then
    echo "Creating $log_folder to store logs."
    mkdir -p $log_folder
fi

# Copy all the generated deployment/platform files into chord folder
cp $BASE_DIR/platforms/* .
cp $BASE_DIR/deployment/* .

file_table="timings_$SGHASH.csv"
host_info="host_info.org"
rm -rf $host_info

# The las %U is just to ease the parsing for table
timefmt="clock:%e user:%U sys:%S telapsed:%e swapped:%W exitval:%x max:%Mk avg:%Kk %U"

# Copy command. This way one can use cp, scp and a local folder or a folder in 
# a cluster.
sep=','
cp_cmd='cp'
dest=$log_folder"/." # change for <user>@<node>.grid5000.fr:~/$log_folder if necessary
###############################################################################

###############################################################################
echo "Recompile the binary against $SGPATH"
export LD_LIBRARY_PATH="$SGPATH/lib"
rm -rf chord
gcc chord.c -L$SGPATH/lib -I$SGPATH/include -I$SGPATH/src/include -lsimgrid -o chord

if [ ! -e "chord" ]; then
    echo "chord does not exist"
    exit;
fi
###############################################################################

###############################################################################
# PRINT HOST INFORMATION IN DIFFERENT FILE
set +e
echo "#+TITLE: Chord experiment on $(eval hostname)" >> $host_info
echo "#+DATE: $(eval date)" >> $host_info
echo "#+AUTHOR: $(eval whoami)" >> $host_info
echo " " >> $host_info 

echo "* People logged when experiment started:" >> $host_info
who >> $host_info
echo "* Hostname" >> $host_info
hostname >> $host_info
echo "* System information" >> $host_info
uname -a >> $host_info
echo "* CPU info" >> $host_info
cat /proc/cpuinfo >> $host_info
echo "* CPU governor" >> $host_info
if [ -f /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor ];
then
    cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor >> $host_info
else
    echo "Unknown (information not available)" >> $host_info
fi
echo "* CPU frequency" >> $host_info
if [ -f /sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq ];
then
    cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq >> $host_info
else
    echo "Unknown (information not available)" >> $host_info
fi
echo "* Meminfo" >> $host_info
cat /proc/meminfo >> $host_info
echo "* Memory hierarchy" >> $host_info
lstopo --of console >> $host_info
echo "* Environment Variables" >> $host_info
printenv >> $host_info
echo "* Tools" >> $host_info
echo "** Linux and gcc versions" >> $host_info
cat /proc/version >> $host_info
echo "** Gcc info" >> $host_info
gcc -v 2>> $host_info 
echo "** Make tool" >> $host_info
make -v >> $host_info
echo "** CMake" >> $host_info
cmake --version >> $host_info
echo "* SimGrid Version" >> $host_info
grep "SIMGRID_VERSION_STRING" ../../../include/simgrid_config.h | sed 's/.*"\(.*\)"[^"]*$/\1/' >> $host_info
echo "* SimGrid commit hash" >> $host_info
git rev-parse --short HEAD >> $host_info
$($cp_cmd $host_info $dest)
###############################################################################

###############################################################################
# ECHO TABLE HEADERS INTO FILE_TABLE
rm -rf $file_table
tabs_needed=""
for thread in "${threads[@]}"; do
thread_line=$thread_line"\t"$thread
done
thread_line=$thread_line$thread_line
for size in $(seq 1 $((${#threads[@]}-1))); do
tabs_needed=$tabs_needed"\t"
done
echo "#SimGrid commit $SGHASH"     >> $file_table 
echo -e "#\t\tconstant${tabs_needed}precise"     >> $file_table
echo -e "#size/thread$thread_line" >> $file_table
###############################################################################

###############################################################################
# START SIMULATION

test -e tmp || mkdir tmp
me=tmp/`hostname -s`

for size in "${sizes[@]}"; do
    line_table=$size
    # CONSTANT MODE
    for thread in "${threads[@]}"; do
        filename="chord_${size}_threads${thread}_constant.log"
        rm -rf $filename

        if [ ! -f  chord$size.xml ]; then
        ./generate.py -p -n $size -b 32 -e 10000
        fi

        if [ ! -f  One_cluster_nobb_${size}_hosts.xml ]; then
        ./generate.py -d -n $size 
        fi


        echo "$size nodes, constant model, $thread threads"
        cmd="./chord One_cluster_nobb_"$size"_hosts.xml chord$size.xml --cfg=contexts/stack_size:16 --cfg=network/model:Constant --cfg=network/latency_factor:0.1 --log=root.thres:info --cfg=contexts/nthreads:$thread --cfg=contexts/guard_size:0"

        /usr/bin/time -f "$timefmt" -o $me.timings $cmd $cmd 1>/tmp/stdout-xp 2>/tmp/stderr-xp

        if grep "Command terminated by signal" $me.timings ; then
            echo "Error detected:"
            temp_time="errSig"
        elif grep "Command exited with non-zero status" $me.timings ; then
            echo "Error detected:"
            temp_time="errNonZero"
        else
            temp_time=$(cat $me.timings | awk '{print $(NF)}')
        fi

        # param
        cat $host_info >> $filename
        echo "* Experiment settings" >> $filename
        echo "size:$size, constant network, $thread threads" >> $filename
        echo "cmd:$cmd" >> $filename
        #stderr
        echo "* Stderr output" >> $filename
        cat /tmp/stderr-xp >> $filename
        # time
        echo "* Timings" >> $filename
        cat $me.timings >> $filename
        line_table=$line_table$sep$temp_time
        $($cp_cmd $filename $dest)
        rm -rf $filename
        rm -rf $me.timings
    done    

    #PRECISE MODE    
    for thread in "${threads[@]}"; do
        echo "$size nodes, precise model, $thread threads"
        filename="chord_${size}_threads${thread}_precise.log"

        cmd="./chord One_cluster_nobb_"$size"_hosts.xml chord$size.xml --cfg=contexts/stack_size:16 --cfg=maxmin/precision:0.00001 --log=root.thres:info --cfg=contexts/nthreads:$thread --cfg=contexts/guard_size:0"

        /usr/bin/time -f "$timefmt" -o $me.timings $cmd $cmd 1>/tmp/stdout-xp 2>/tmp/stderr-xp

        if grep "Command terminated by signal" $me.timings ; then
            echo "Error detected:"
            temp_time="errSig"
        elif grep "Command exited with non-zero status" $me.timings ; then
            echo "Error detected:"
            temp_time="errNonZero"
        else
            temp_time=$(cat $me.timings | awk '{print $(NF)}')
        fi
        # param
        cat $host_info >> $filename
        echo "* Experiment settings" >> $filename
        echo "size:$size, constant network, $thread threads" >> $filename
        echo "cmd:$cmd" >> $filename
        #stderr
        echo "* Stderr output" >> $filename
        cat /tmp/stderr-xp >> $filename
        # time
        echo "* Timings" >> $filename
        cat $me.timings >> $filename
        line_table=$line_table$sep$temp_time
        $($cp_cmd $filename $dest)
        rm -rf $filename
        rm -rf $me.timings
    done

    echo -e $line_table >> $file_table

done

$($cp_cmd $file_table $dest)
rm -rf $file_table
rm -rf tmp
#+end_src

#+name: testall_sr
#+begin_src sh  :var SG_PATH='/usr/local' :var log_folder="logs" :session org-sh
# This script is to benchmark the Chord simulation that can be found
# in examples/msg/chord folder.
# The benchmark is done with both Constant and Precise mode, using
# different sizes and number of threads (which can be modified).
# This script also generate a table with all the times gathered, that can ease
# the plotting, compatible with gnuplot/R.
# By now, this script copy all data (logs generated an final table) to a 
# personal frontend-node in Grid5000. This should be modified in the near
# future.

###############################################################################
# MODIFIABLE PARAMETERS: SGPATH, SGHASH, sizes, threads, log_folder, file_table
# host_info, timefmt, cp_cmd, dest.

# Path to installation folder needed to recompile chord
# If it is not set, assume that the path is '/usr/local'
if [ -z "$SG_PATH" ]
then
    SGPATH='/usr/local'
fi

# Save the revision of SimGrid used for the experiment
SGHASH=$(git rev-parse --short HEAD)

# List of sizes to test. Modify this to add different sizes.
if [ -z "$sizes" ]
then
    sizes=(1000 3000)
fi

# Number of threads to test. 
if [ -z "$threads"]
then
    threads=(1 2 4 8 16 24)
fi

# Path where to store logs, and filenames of times table, host info
if [ -z "$log_folder"]
then
    log_folder=$BASE_DIR"/logs"
else
    log_folder=$BASE_DIR"/logs/"$log_folder
fi

if [ ! -d "$log_folder" ]
then
    echo "Creating $log_folder to store logs."
    mkdir -p $log_folder
fi

# Copy all the generated deployment/platform files into chord folder
cp $BASE_DIR/platforms/* .
cp $BASE_DIR/deployment/* .

file_table="timings_$SGHASH.csv"
host_info="host_info.org"
rm -rf $host_info

# The las %U is just to ease the parsing for table
timefmt="clock:%e user:%U sys:%S telapsed:%e swapped:%W exitval:%x max:%Mk avg:%Kk %U"

# Copy command. This way one can use cp, scp and a local folder or a folder in 
# a cluster.
sep=','
cp_cmd='cp'
dest=$log_folder # change for <user>@<node>.grid5000.fr:~/$log_folder if necessary
###############################################################################

###############################################################################
echo "Recompile the binary against $SGPATH"
export LD_LIBRARY_PATH="$SGPATH/lib"
rm -rf chord
gcc chord.c -L$SGPATH/lib -I$SGPATH/include -I$SGPATH/src/include -lsimgrid -o chord

if [ ! -e "chord" ]; then
    echo "chord does not exist"
    exit;
fi
###############################################################################

###############################################################################
# PRINT HOST INFORMATION IN DIFFERENT FILE
set +e
echo "#+TITLE: Chord experiment on $(eval hostname)" >> $host_info
echo "#+DATE: $(eval date)" >> $host_info
echo "#+AUTHOR: $(eval whoami)" >> $host_info
echo " " >> $host_info 

echo "* People logged when experiment started:" >> $host_info
who >> $host_info
echo "* Hostname" >> $host_info
hostname >> $host_info
echo "* System information" >> $host_info
uname -a >> $host_info
echo "* CPU info" >> $host_info
cat /proc/cpuinfo >> $host_info
echo "* CPU governor" >> $host_info
if [ -f /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor ];
then
    cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor >> $host_info
else
    echo "Unknown (information not available)" >> $host_info
fi
echo "* CPU frequency" >> $host_info
if [ -f /sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq ];
then
    cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq >> $host_info
else
    echo "Unknown (information not available)" >> $host_info
fi
echo "* Meminfo" >> $host_info
cat /proc/meminfo >> $host_info
echo "* Memory hierarchy" >> $host_info
lstopo --of console >> $host_info
echo "* Environment Variables" >> $host_info
printenv >> $host_info
echo "* Tools" >> $host_info
echo "** Linux and gcc versions" >> $host_info
cat /proc/version >> $host_info
echo "** Gcc info" >> $host_info
gcc -v 2>> $host_info 
echo "** Make tool" >> $host_info
make -v >> $host_info
echo "** CMake" >> $host_info
cmake --version >> $host_info
echo "* SimGrid Version" >> $host_info
grep "SIMGRID_VERSION_STRING" ../../../include/simgrid_config.h | sed 's/.*"\(.*\)"[^"]*$/\1/' >> $host_info
echo "* SimGrid commit hash" >> $host_info
git rev-parse --short HEAD >> $host_info
$($cp_cmd $host_info $dest)
###############################################################################

###############################################################################
# ECHO TABLE HEADERS INTO FILE_TABLE
rm -rf $file_table
tabs_needed=""
for thread in "${threads[@]}"; do
thread_line=$thread_line"\t"$thread
done
thread_line=$thread_line$thread_line
for size in $(seq 1 $((${#threads[@]}-1))); do
tabs_needed=$tabs_needed"\t"
done
echo "#SimGrid commit $SGHASH"     >> $file_table 
echo -e "#\t\tconstant${tabs_needed}precise"     >> $file_table
echo -e "#size/thread$thread_line" >> $file_table
###############################################################################

###############################################################################
# START SIMULATION

test -e tmp || mkdir tmp
me=tmp/`hostname -s`

for size in "${sizes[@]}"; do
    line_table=$size
    # CONSTANT MODE
    for thread in "${threads[@]}"; do
        filename="chord_${size}_threads${thread}_constant.log"
    	output="sr_${size}_threads${thread}_constant.log"
        rm -rf $filename

        if [ ! -f  chord$size.xml ]; then
        ./generate.py -p -n $size -b 32 -e 10000
        fi

        if [ ! -f  One_cluster_nobb_${size}_hosts.xml ]; then
        ./generate.py -d -n $size 
        fi


        echo "$size nodes, constant model, $thread threads"
        cmd="./chord One_cluster_nobb_"$size"_hosts.xml chord$size.xml --cfg=contexts/stack_size:16 --cfg=network/model:Constant --cfg=network/latency_factor:0.1 --log=root.thres:critical --cfg=contexts/nthreads:$thread --cfg=contexts/guard_size:0"

        /usr/bin/time -f "$timefmt" -o $me.timings $cmd $cmd 1>/tmp/stdout-xp 2>/tmp/stderr-xp

        if grep "Command terminated by signal" $me.timings ; then
            echo "Error detected:"
            temp_time="errSig"
        elif grep "Command exited with non-zero status" $me.timings ; then
            echo "Error detected:"
            temp_time="errNonZero"
        else
            temp_time=$(cat $me.timings | awk '{print $(NF)}')
        fi

        # param
        cat $host_info >> $filename
        echo "* Experiment settings" >> $filename
        echo "size:$size, constant network, $thread threads" >> $filename
        echo "cmd:$cmd" >> $filename
        #stdout
        echo "* Stdout output" >> $filename
        cat /tmp/stdout-xp | grep Amdahl >> $filename
        #stderr
        echo "* Stderr output" >> $filename
        cat /tmp/stderr-xp >> $filename
        # time
        echo "* Timings" >> $filename
        cat $me.timings >> $filename
        line_table=$line_table$sep$temp_time
        # Gather SR data from logs
        echo -e '#id_sr\ttime_taken\tamount_proccesses' >> $output
        grep 'Total time SR' $filename | awk '{print $7 "\x09" $9 "\x09" $10}' | tr -d ',' >> $output
        $($cp_cmd $output $dest)
        $($cp_cmd $filename $dest)
        rm -rf $filename $output
        rm -rf $me.timings
    done    

    #PRECISE MODE    
    for thread in "${threads[@]}"; do
        echo "$size nodes, precise model, $thread threads"
        filename="chord_${size}_threads${thread}_precise.log"
    	output="sr_${size}_threads${thread}_precise.log"

        cmd="./chord One_cluster_nobb_"$size"_hosts.xml chord$size.xml --cfg=contexts/stack_size:16 --cfg=maxmin/precision:0.00001 --log=root.thres:critical --cfg=contexts/nthreads:$thread --cfg=contexts/guard_size:0"

        /usr/bin/time -f "$timefmt" -o $me.timings $cmd $cmd 1>/tmp/stdout-xp 2>/tmp/stderr-xp

        if grep "Command terminated by signal" $me.timings ; then
            echo "Error detected:"
            temp_time="errSig"
        elif grep "Command exited with non-zero status" $me.timings ; then
            echo "Error detected:"
            temp_time="errNonZero"
        else
            temp_time=$(cat $me.timings | awk '{print $(NF)}')
        fi
        # param
        cat $host_info >> $filename
        echo "* Experiment settings" >> $filename
        echo "size:$size, constant network, $thread threads" >> $filename
        echo "cmd:$cmd" >> $filename
        #stderr
        echo "* Stderr output" >> $filename
        cat /tmp/stderr-xp >> $filename
        # time
        echo "* Timings" >> $filename
        cat $me.timings >> $filename
        line_table=$line_table$sep$temp_time
        # Gather SR data from logs
        echo -e '#id_sr\ttime_taken\tamount_proccesses' >> $output
        grep 'Total time SR' $filename | awk '{print $7 "\x09" $9 "\x09" $10}' | tr -d ',' >> $output
        $($cp_cmd $output $dest)
        $($cp_cmd $filename $dest)
        rm -rf $filename $output
        rm -rf $me.timings
    done
    echo -e $line_table >> $file_table
done

$($cp_cmd $file_table $dest)
rm -rf $file_table
rm -rf tmp
#+end_src

** Amdahl speedup
The benchmark can be run from this org-mode file, or simply by running
./scripts/chord/testall.sh (in this repository) inside the folder
examples/msg/chord of your SimGrid installation. Inside that script,
the number of threads to test, as well as the amount of nodes, can be
modified

The constant TIME_BENCH_AMDAHL must be defined in SimGrid in order to
enable the required logs for this experiment. This variable can be
defined in the file src/simix/smx_private.h

The script generates a .csv table, but just in case it is done in
different stages, the gathered logs can be processed with
get_times.py, located in the same folder as testall.sh. This generates
a .csv that can easily be plotted with R/gnuplot. 

The script is self-documented.

#+call: check_args[:session org-sh]()

#+call: go_to_chord[:session org-sh]()

#+call: testall[:session org-sh](log_folder='timings/logs')

** SR Distribution
To enable Scheduling Rounds benchmarks, the constant TIME_BENCH_PER_SR
has to be defined. It can be defined in src/simix/smx_private.h
The logs give information about the time it takes to run a scheduling
round, as well as the amount of processes each SR takes.
For this experiment, we are only interested in the amount of processes
taken by each SR.

The script to run this experiment is ./scripts/chord/testall_sr.sh,
the id of SR, time of SR and num processes of SR, in a table format.

This can be run from here or just by running testall_sr.sh in in the
examples/msg/chord folder of your SimGrid install.

#+call: check_args[:session org-sh]()

#+call: go_to_chord[:session org-sh]()

#+call: testall_sr[:session org-sh](log_folder='sr_counts/logs')

** SR Times
The data set used for this plot is the same as the one before.
We just use the data of the sequential simulations (1 thread).
** Binding threads to physical cores
The constant CORE_BINDING has to be defined in src/xbt/parmap.c in
order to enable this optimization.  The benchmark is then run in the
same way as the Amdahl Speedup experiment.

#+call: check_args[:session org-sh]()

#+call: go_to_chord[:session org-sh]()

#+call: testall[:session org-sh](log_folder='binding_cores/logs')
** parmap between N cores
This may be the experiment that requires more work to reproduce:
*** maestro works with N-1 threads
This is the default setting and the standard benchmark can be used.

#+call: check_args[:session org-sh]()

#+call: go_to_chord[:session org-sh]()

#+call: testall[:session org-sh](log_folder='pmapM_N-1/logs')

*** maestro sleeps with N-1 threads
To avoid that maestro works with the threads, comment out the line:
    xbt_parmap_work(parmap);
from the function xbt_parmap_apply() in src/xbt/parmap.c

#+call: check_args[:session org-sh]()

#+call: go_to_chord[:session org-sh]()

#+call: testall[:session org-sh](log_folder='pmap_N-1/logs')

*** maestro sleeps with N threads
To avoid that maestro works with the threads, comment out the line:
    xbt_parmap_work(parmap);
from the function xbt_parmap_apply() in src/xbt/parmap.c
Then the function src/xbt/parmap.c:xbt_parmap_new has to be
modified to create one extra thread. It is easy: just add 1 to
num_workers parameter.

#+call: check_args[:session org-sh]()

#+call: go_to_chord[:session org-sh]()

#+call: testall[:session org-sh](log_folder='pmap_N/logs')

** Busy Waiters vs. Futexes performance
Enable the use of busy waiters running chord with the extra option:
    --cfg=contexts/synchro:busy_wait
The experiment was run with testall.sh using that extra option in the
chord command inside the script. The tables were constructed using get_times.py
The data regarding the futexes synchro times is the same gathered in Amdahl
Speedup experiment.

#+call: check_args[:session org-sh]()

#+call: go_to_chord[:session org-sh]()

#+call: testall[:session org-sh](log_folder='busy_waiters/logs')

** Performance Regression Testing
** SR parallel threshold
The data set is the same as SR Distribution and SR times experiments.
** Adaptive Algorithm
The benchmark is done using testall.sh. The algorithm is the one
described in section 5.2

#+call: check_args[:session org-sh]()

#+call: go_to_chord[:session org-sh]()

#+call: testall[:session org-sh](log_folder='adaptive-algorithm/logs')

* Data Analysis                                                    :noexport: 
** Installing required packages
#+begin_src R :exports none
install.packages("ggplot2")
install.packages("gridExtra")
install.packages("reshape")
install.packages("plyr")
install.packages("data.table")
install.packages("stringr")
install.packages("grid")
#+end_src

** Libraries/Auxiliary functions
#+begin_src R  :exports none
# If you miss the libraries, try typing >>>install.packages("data.table")<<< in a R console
library('ggplot2')
library('gridExtra')
library('reshape')
library('plyr')
library('data.table')
library('stringr')
require('grid')
# To plot several ggplot in one window.
vp.layout <- function(x, y) viewport(layout.pos.row=x, layout.pos.col=y)
arrange_ggplot2 <- function(..., nrow=NULL, ncol=NULL, as.table=FALSE) {
    dots <- list(...)
    n <- length(dots)
    if(is.null(nrow) & is.null(ncol)){
        nrow = floor(n/2) ; ncol = ceiling(n/nrow)
    }
    if(is.null(nrow)){
        nrow = ceiling(n/ncol)
    }
    if(is.null(ncol)){
        ncol = ceiling(n/nrow)
    }
    grid.newpage()
    pushViewport(viewport(layout=grid.layout(nrow,ncol)))
    ii.p <- 1
    for(ii.row in seq(1, nrow)){
        ii.table.row <- ii.row
        if(as.table) {
            ii.table.row <- nrow - ii.table.row + 1
        }
        for(ii.col in seq(1, ncol)){
            ii.table <- ii.p
            if(ii.p > n) break
            print(dots[[ii.table]], vp=vp.layout(ii.table.row, ii.col))
            ii.p <- ii.p + 1
        }
    }
}

# Get legend from a given plot
g_legend<-function(a.gplot){
    tmp <- ggplot_gtable(ggplot_build(a.gplot))
    leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
    legend <- tmp$grobs[[leg]]
    return(legend)
}
#+end_src

** Pre-processing of datasets
Here we pre process some data to make the plotting stage faster. Also,
the .csv files needed for almost all plots are created here.

#+name: process_data_sr-times
#+begin_src R
temp = list.files(path='./logs/sr_counts/sequential', pattern="*precise*", full.names = TRUE)
flist <- lapply(temp, read.table)
sr_data <- rbindlist(flist)
sr_data[, "V1"] <- NULL
sr_data = as.data.frame.matrix(sr_data)
saveRDS(sr_data, file="./logs/sr_counts/sr-times.Rda")
#+end_src

#+name: process_data_sr-par-threshold
#+begin_src R
#PRECISE MODE
#SEQUENTIAL
temp = list.files(path='../code_chunks/other_logs/sr_counts/new', pattern="threads1_", full.names = TRUE)
temp <- temp[grepl("precise", temp)]
temp <- temp[grepl("25000", temp)]
#temp <- temp[-grep("50000", temp)]
#temp <- temp[-grep("75000", temp)]
flist <- lapply(temp, read.table)
sr_data <- rbindlist(flist)
#sr_data[, "V1"] <- NULL
sr_data = as.data.frame.matrix(sr_data)
#df <- ddply(sr_data, .(V3), summarize, mean_value = mean(V2))

#PARALLEL:
temp2 = list.files(path='../code_chunks/other_logs/sr_counts/new', pattern="threads4_", full.names = TRUE)
temp2 <- temp2[grepl("precise", temp2)]
temp2 <- temp2[grepl("25000", temp2)]
flist2 <- lapply(temp2, read.table)
sr_data2 <- rbindlist(flist2)
#sr_data2[, "V1"] <- NULL
sr_data2 = as.data.frame.matrix(sr_data2)
#df2 <- ddply(sr_data2, .(V3), summarize, mean_value = mean(V2))

#CONSTANT MODE
#SEQUENTIAL
#temp3 = list.files(path='./logs/sr_counts/sequential', pattern="threads4_", full.names = TRUE)
#temp3 <- temp3[grepl("constant", temp3)]
#flist <- lapply(temp3, read.table)
#sr_data3 <- rbindlist(flist)
#sr_data[, "V1"] <- NULL
sr_data3 = as.data.frame.matrix(sr_data3)
#df3 <- ddply(sr_data3, .(V3), summarize, mean_value = mean(V2))


#PARALLEL:
#temp4 = list.files(path='./logs/sr_counts/parallel', pattern="threads4_", full.names = TRUE)
#temp4 <- temp4[grepl("constant", temp4)]
#temp4 <- temp4[-grep("50000", temp4)]
#temp4 <- temp4[-grep("75000", temp4)]
#flist2 <- lapply(temp4, read.table)
#sr_data4 <- rbindlist(flist2)
#sr_data2[, "V1"] <- NULL
#sr_data4 = as.data.frame.matrix(sr_data4)
#df4 <- ddply(sr_data4, .(V3), summarize, mean_value = mean(V2))

#Merge PRECISE datasets
df5 = merge(sr_data, sr_data2, by = 'V1', incomparables = NULL)
df5 <- transform(df5, speedup = V2.x / V2.y)
#df5[, 'speedup'] <- df5[,'mean_value.x'] / df5[, 'mean_value.y']
saveRDS(df5, file="./logs/sr_counts/precise.Rda")
#Merge CONSTANT datasets
#df6 = merge(sr_data3, sr_data4, by = 'V1', incomparables = NULL)
#df6 <- transform(df6, speedup = V2.x / V2.y)
#df6[, 'speedup'] <- df6[,'mean_value.x'] / df6[, 'mean_value.y']
#saveRDS(df6,file="./logs/sr_counts/constant.Rda")
#+end_src

# OPTIONAL: Maybe you want to call this function to be sure that the THREADS and SIZES are the ones you want to plot.
#+call: set_python_args() :session

#+name: create_table
#+begin_src python :session :var elapsed=0 :var amdahl=0 :var memory=0 :var logs_path='"logs"' :var output_file='"logs/total_times.csv"' :results output
  # This is a set of functions that can generate nice .csv files with
  # the times of the experiments. Also, the memory consumption can be
  # gathered. Note that the logs are the ones generated by [[testall]]
  # code chunk.

  #Parameters: elapsed: if set to True, then the elapsed time (wallclock) is gathered.
  #            amdahl:  if set to True, then the times of the Amdahl benchmark are gathered.
  #            memory:  if set to True, then the peak RAM used by the process is gathered.
  #               If none of them is gathered, then the usrtime + systime is gathered.
  #            logs_path: where are stored the logs to analyze.
  #            output_file_path: where to store the produced table

  # If you make several test of the same experiment, you can name the log files
  # with a prefix ('1_chord..., 2_chord...') and then put the prefixes
  # you used in input_seq. The script will average the corresponding values
  # for you.
  input_seq = ['']


  def parse_elapsed_and_memory_used(file):
      line = file.read().splitlines()
      l = line[-1]
      if l:
          t = float((l.split()[0]).split(':')[1])
          mem = float(((l.split()[6]).split(':')[1]).replace('k', ''))
          mem = mem / (1024.0 * 1024.0)  # gigabytes used
          mem = float(("{0:.2f}".format(mem)))
          return (t, mem)
      else:
          return (0, 0)


  def parse_memory_used(file):
      line = file.read().splitlines()
      l = line[-1]
      if l:
          mem = float(((l.split()[6]).split(':')[1]).replace('k', ''))
          mem = mem / (1024.0 * 1024.0)  # gigabytes used
          mem = float(("{0:.2f}".format(mem)))
          return mem
      else:
          return 0


  def parse_elapsed_real(file):
      line = file.read().splitlines()[-1]
      if line:
          return float((line.split()[0]).split(':')[1])
      else:
          return 0


  def parse_user_kernel(file):
      line = file.read().splitlines()[-1]
      if line:
          usrtime = float((line.split(":")[2]).split()[0])
          systime = float((line.split(":")[3]).split()[0])
          return usrtime + systime


  def parse_amdahl_times(file):
      line = [line for line in file.read().splitlines() if "Amdahl" in line]
      line = [(((l.split(";")[0]).split(":")[-1]).strip(),
              ((l.split(";")[1]).split(":")[1]).strip())
              for l in line][0]
      return float(line[0]) + float(line[1])


  def print_header(file):
      file.write('"nodes"')
      for mode in MODES:
          for thread in THREADS:
              file.write(',"'+mode[0]+str(thread)+'"')
      file.write('\n')


  def parse_files(elapsed, amdahl, mem, logs_path, output_file):
      f = open(output_file, "w")
      print_header(f)
      for size in SIZES:
          temp_line = "{}".format(size)
          for mode in MODES:
              for thread in THREADS:
                  sum_l = 0.
                  mem_used = 0.
                  leng = len(input_seq)
                  for seq in input_seq:
                      file = open("{}/chord{}_{}_threads{}_{}.log".format(logs_path,
                                  seq, size, thread, mode), "r")
                      if mem and elapsed:
                          tup = parse_elapsed_and_memory_used(file)
                          sum_l += tup[0]
                          mem_used += tup[1]
                      elif elapsed:
                          sum_l += parse_elapsed_real(file)
                      elif amdahl:
                          sum_l += parse_amdahl_times(file)
                      elif mem:
                          sum_l += parse_memory_used(file)
                      else:
                          sum_l += parse_user_kernel(file)
                  if leng != 0:
                      if mem and elapsed:
                          temp_line += ",{0},{1:.2f}".format(datetime.timedelta(seconds=int(sum_l / float(leng))),
                                                             (mem_used / float(leng)))
                      else:
                          temp_line += ",{}".format(sum_l / float(leng))
                  else:
                      if mem and elapsed:
                          temp_line += ",?,?"
                      else:
                          temp_line += ",?"
          f.write(temp_line + "\n")
      f.close()

  parse_files(elapsed, amdahl, memory, logs_path, output_file)
#+end_src

#+call: create_table(0,1,0,'"logs/amdahl/logs"','"logs/amdahl/total_times_amdahl.csv"') :session
#+call: create_table(1,0,0,'"logs/timings/logs"','"logs/timings/total_times.csv"') :session
#+call: create_table(0,0,1,'"logs/timings/logs"','"logs/timings/memory_consumption.csv"') :session

# Call this to change the amount of threads: in the next 2 tables, we dont take the serial benchmarks.
#+begin_src python :session
  # We only test performance improvements in parallel executions with
  # adaptive algorithm and busy_waiters.
  THREADS = [2, 4, 8, 16, 24]
#+end_src

#+call: create_table(1,0,0,'"logs/busy_waiters/logs"','"logs/busy_waiters/total_times_busy.csv"')  :session
#+call: create_table(1,0,0,'"logs/adaptive_algorithm/logs"','"logs/adaptive_algorithm/total_times_adaptive.csv"') :session

# OPTIONAL: This csv is useful for the table of Section 5.2

#+call: create_table(1,0,1,'"logs/timings/logs"','"logs/timings/total_times_memory_adaptive.csv"') :session

#+RESULTS:

** Plotting
#+name: baseline_perf
#+begin_src R  :results output graphics :exports results :file fig/baseline-perf.pdf
data = read.csv("./logs/timings/total_times.csv", head=TRUE, sep=',')

# Speedups of Precise Mode
data[, "1"] <- data[, "p1"]  / data[, "p1"]
data[, "2"] <- data[, "p1"]  / data[, "p2"]
data[, "4"] <- data[, "p1"]  / data[, "p4"]
data[, "8"] <- data[, "p1"]  / data[, "p8"]
data[, "16"] <- data[,"p1"]  / data[, "p16"]
data[, "24"] <- data[,"p1"]  / data[, "p24"]
keep <- c("nodes", colnames(data)[grep("^[1-9]", colnames(data))])
speedup_precise <- data[keep]
df2 <- melt(speedup_precise ,  id = 'nodes', variable_name = 'threads')
g2<-ggplot(df2, aes(x=nodes,y=value, group=threads, colour=threads)) + geom_line() + theme(axis.text.x = element_text(angle = -45, hjust = 0),
    panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(),
    legend.position="right") + xlab("Amount of nodes simulated") + ylab("Speedup-Precise mode") + 
    scale_fill_discrete(name="threads")
g2
#+end_src

#+name: sr-distribution
#+begin_src R :results output graphics :exports results :file fig/sr-distribution.pdf
temp = list.files(path='./logs/sr_counts/new/parallel2', pattern="threads4", full.names = TRUE)
temp <- temp[grep("precise",temp)]
# This data.frame will store the final proportion values.
#proportions <- data.frame(stringsAsFactors=FALSE)
proportions <- data.frame(row.names = c('1','2','3-5','6-10','11-20','21-30','31+'))
head <- c()
for(i in temp){
    col <- c()
    # Parse amount of nodes from the file path.
    # Example of file path: './logs/sr_counts/parallel/sr_10000_threads4_constant.log'
    nodes = strsplit(str_extract(i, "_[0-9]+_"), "_")[[1]][2]
    head <- c(head,as.numeric(nodes))
    col <- c(col, nodes)
    # Keep only the column with the amount of processes
    data <- read.table(i)["V3"]
    # Calculate proportions
    data <- prop.table(xtabs(~ V3, data=data))
    # Populate a new data frame with percentages of interest (1, 2, 3 or more processes)
    proc1 <- data["1"][[1]]
    proc2 <- data["2"][[1]]
    proc3_5 <- c(data["3"][[1]],data["4"][[1]], data["5"][[1]])
    proc6_10 <- c(data["6"][[1]], data["7"][[1]], data["8"][[1]], data["9"][[1]], data["10"][[1]])
    proc11_20 <- c(data["11"][[1]], data["12"][[1]], data["13"][[1]], data["14"][[1]], data["15"][[1]], data["16"][[1]], data["17"][[1]], data["18"][[1]], data["19"][[1]], data["20"][[1]])
    proc21_30 <- c(data["21"][[1]], data["22"][[1]], data["23"][[1]], data["24"][[1]], data["25"][[1]], data["26"][[1]], data["27"][[1]], data["28"][[1]], data["29"][[1]], data["30"][[1]])
    # Calculate final percentages and omit any possible NA
    proc3_5 <- Reduce("+", proc3_5[!is.na(proc3_5)])
    proc6_10 <- Reduce("+", proc6_10[!is.na(proc6_10)])
    proc11_20 <- Reduce("+", proc11_20[!is.na(proc11_20)])
    proc21_30 <- Reduce("+", proc21_30[!is.na(proc21_30)])
    proc31 <- 1 - (proc1 + proc2 + proc3_5 + proc6_10 + proc11_20 + proc21_30)
    #p <- c(nodes, proc1, proc2, proc3_5, proc6_10, proc11_20, proc21_30, proc31)
    # And bind to existing data.frame
    #p <- as.data.frame(p)
    #p[,'nodes'] <- nodes
    #p[,'process'] <- c("1","2",">3")
    proportions <- cbind(proportions, nodes = c(proc1, proc2, proc3_5, proc6_10, proc11_20, proc21_30, proc31))
    colnames(proportions)[length(proportions)] <- as.numeric(nodes)
}
head <- sort(head)
cols <- c()
for(e in head){ cols <- c(cols,toString(e))}
proportions <- proportions[,cols]
b <- barplot(as.matrix(proportions), ylab="Proportion of SR's",
legend=rownames(proportions), args.legend = list(x = ncol(proportions) + 5.5, bty = "n"),
xlim=c(0, ncol(proportions) + 4), las=2, cex.axis = 0.8)
title(xlab = "Amount of nodes simulated", line=4)

#df <- ddply(proportions, .(nodes,process), summarise, msteps = mean(p))
#g<-ggplot(df, aes(x=nodes, y=msteps, group=process, colour=process)) + geom_line() +
#   theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
#   panel.background = element_blank(), axis.line=element_line()) +
#   scale_fill_discrete(name="threads") +
#   xlab("Amount of nodes simulated") + ylab("Percentage of SR's containing 1,2 or >3 processes")
#g
#+end_src

#+name: sr-times
#+begin_src R  :results output graphics :exports results  :file fig/sr-times.pdf
sr_data <- readRDS(file="./logs/sr_counts/sr-times.Rda")
#df <- ddply(sr_data, .(V3), summarize, mean_value = mean(V2))
ggplot(data=sr_data, geom="histogram", aes(x=V3, y=V2)) + xlim(0,500) + xlab("Processes in SR") + ylab("Time consumed to compute SR's") + ylim(0,0.005) + geom_point(size = 1) # Replace V2 for 'mean_value' if dont want to plot every dot; and uncomment line above.
#+end_src

#+name: busy
#+begin_src R :results output graphics :exports results :file fig/busy.pdf
orig_data = read.csv("./logs/timings/total_times.csv", head=TRUE, sep=',')
opt_data = read.csv("./logs/busy_waiters/total_times_busy.csv", head=TRUE, sep=',')

# Speedups of Precise Mode
opt_data[, "1"]  <- orig_data[, "p2"]  / orig_data[, "p2"]
opt_data[, "2"]  <- orig_data[, "p2"]  / opt_data[, "p2"]
opt_data[, "4"]  <- orig_data[, "p4"]  / opt_data[, "p4"]
opt_data[, "8"]  <- orig_data[, "p8"]  / opt_data[, "p8"]
opt_data[, "16"] <- orig_data[, "p16"] / opt_data[, "p16"]
opt_data[, "24"] <- orig_data[, "p24"] / opt_data[, "p24"]
keep <- c("nodes", colnames(opt_data)[grep("^[1-9]", colnames(opt_data))])
speedup_precise <- opt_data[keep]

df2 <- melt(speedup_precise ,  id = 'nodes', variable_name = 'threads')

g2<-ggplot(df2, aes(x=nodes,y=value, group=threads, colour=threads)) + geom_line() +
    scale_fill_hue() + theme(axis.text.x = element_text(angle = -45, hjust = 0),
    panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(),
    legend.position="right") + 
    scale_x_continuous(breaks=c(1000,5000,10000,25000,50000,75000,100000)) +
    scale_y_continuous(breaks=c(1.0,1.5,2.0,2.5,3.0,4.0)) + ylab("Speedup-Precise mode") +
    xlab("Amount of nodes simulated")
g2
#+end_src

#+name: sr-par-threshold
#+begin_src R :results output graphics :exports results   :file fig/sr-par-threshold.pdf
precise <- readRDS(file="./logs/sr_counts/precise.Rda")

ggplot(data=precise, geom="histogram", aes(x=V3.x, y=speedup)) +geom_point() + xlim(1,500) +ylim(0,2)+ stat_smooth(se=FALSE) + theme(panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(),
    legend.position="none") + ylab("Speedup of parallel execution against sequential execution") + xlab("Amount of processes computed by each SR")
#+end_src

#+name: adapt-algorithm
#+begin_src R  :results output graphics :exports results  :file fig/adapt-algorithm.pdf
orig_data = read.csv("./logs/timings/total_times.csv")
opt_data = read.csv("./logs/adaptive_algorithm/total_times_adaptive.csv")

# Speedups of Precise Mode
opt_data[, "1"]  <- orig_data[, "p2"]  / orig_data[, "p2"]
opt_data[, "2"]  <- orig_data[, "p2"]  / opt_data[, "p2"]
opt_data[, "4"]  <- orig_data[, "p4"]  / opt_data[, "p4"]
opt_data[, "8"]  <- orig_data[, "p8"]  / opt_data[, "p8"]
opt_data[, "16"] <- orig_data[, "p16"] / opt_data[, "p16"]
opt_data[, "24"] <- orig_data[, "p24"] / opt_data[, "p24"]
keep <- c("nodes", colnames(opt_data)[grep("^[1-9]", colnames(opt_data))])
speedup_precise <- opt_data[keep]

df2 <- melt(speedup_precise ,  id = 'nodes', variable_name = 'threads')

g2<-ggplot(df2, aes(x=nodes,y=value, group=threads, colour=threads)) + geom_line() + scale_fill_hue() + theme(axis.text.x = element_text(angle = -45, hjust = 0),
    panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(),
    legend.position="right") +
    scale_x_continuous(breaks=c(1000,5000,10000,25000,50000,75000)) + 
    scale_y_continuous(breaks=c(1,2,3,4)) + ylab("Speedup-Precise mode") +
    xlab("Amount of nodes simulated")
g2
#+end_src

#+name: memory-consumption
#+begin_src R :results output graphics :exports results  :file fig/memory-consumption.pdf
data = read.csv("./logs/timings/memory_consumption.csv", head=TRUE, sep=',')
df2 <- melt(data,  id = 'nodes', variable_name = 'threads')
g2<-ggplot(df2, aes(x=nodes,y=value, group=threads, colour=threads)) + geom_line() + theme(axis.text.x = element_text(angle = -45, hjust = 0),
    panel.grid.major=element_line(colour='grey'),panel.grid.minor=element_blank(),
    panel.background = element_blank(), axis.line=element_line(),
    legend.position="right") + xlab("Amount of nodes simulated") + ylab("Memory Consumption (GB)") + 
    scale_fill_discrete(name="threads") + scale_color_manual(values=c('brown1','darkblue','darkorange2','cadetblue2','gold','hotpink4'),labels = c("1","2","4","8","16","24"))
g2
#+end_src


* Emacs Setup                                                      :noexport:
  This document has local variables in its postembule, which should
  allow org-mode to work seamlessly without any setup. If you're
  uncomfortable using such variables, you can safely ignore them at
  startup. Exporting may require that you copy them in your .emacs.

# Local Variables:
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (sh . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes 
#                       '("sigalt" "\\documentclass{sig-alternate}"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")))
# eval:    (add-hook 'org-babel-after-execute-hook 'org-display-inline-images) 
# eval:    (add-hook 'org-mode-hook 'org-display-inline-images)
# eval:    (add-hook 'org-mode-hook 'org-babel-result-hide-all)
# eval:   (setq org-babel-default-header-args:R '((:session . "org-R")))
# eval:   (setq org-export-babel-evaluate nil)
# eval:   (setq org-latex-to-pdf-process '("pdflatex -interaction nonstopmode -output-directory %o %f ; bibtex `basename %f | sed 's/\.tex//'` ; pdflatex -interaction nonstopmode -output-directory  %o %f ; pdflatex -interaction nonstopmode -output-directory %o %f"))
# eval:   (setq ispell-local-dictionary "american")
# eval:    (setq org-export-latex-table-caption-above nil)
# eval:   (eval (flyspell-mode t))
# End:

